{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fe347ff",
   "metadata": {},
   "source": [
    "#### Simple Gen AI APP Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f667d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACKING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab8fdc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x10c4816d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Ingestion == from the. website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/evaluation/tutorials/rag\")\n",
    "loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "449292a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nEvaluate a RAG application | 🦜️🛠️ LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationQuick StartTutorialsEvaluate a chatbotEvaluate a RAG applicationRun backtests on a new version of an agentRunning SWE-bench with LangSmithEvaluate a complex agentImproving LLM-as-judge evaluators using human feedbackTest a ReAct agent with Pytest/Vitest and LangSmithHow-to GuidesAnalyze a single experimentLog user feedback using the SDKHow to run an evaluationCreating and Managing Datasets in the UIRenaming an experimentAutomatically run evaluators on experimentsHow to manage datasets programmaticallyRunning an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate an application\\'s intermediate stepsHow to version a datasetUse annotation queuesDataset SharingHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to run evals with pytest (beta)Run pairwise evaluationsHow to audit evaluator scoresHow to improve your evaluator with few-shot examplesHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to define a target function to evaluateHow to download experiment results as a CSVRun an evaluation with multimodal contentHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to run an evaluation locally (Python only)How to return categorical vs numerical metricsHow to simulate multi-turn interactionsHow to return multiple scores in one evaluatorHow to use prebuilt evaluatorsHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorHow to run evals with Vitest/Jest (beta)Conceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationTutorialsEvaluate a RAG applicationOn this pageEvaluate a RAG application\\nKey conceptsRAG evaluation | Evaluators | LLM-as-judge evaluators\\nRetrieval Augmented Generation (RAG) is a technique that enhances Large Language Models (LLMs) by providing them with relevant external knowledge. It has become one of the most widely used approaches for building LLM applications.\\nThis tutorial will show you how to evaluate your RAG applications using LangSmith. You\\'ll learn:\\n\\nHow to create test datasets\\nHow to run your RAG application on those datasets\\nHow to measure your application\\'s performance using different evaluation metrics\\n\\nOverview\\u200b\\nA typical RAG evaluation workflow consists of three main steps:\\n\\nCreating a dataset with questions and their expected answers\\nRunning your RAG application on those questions\\nUsing evaluators to measure how well your application performed, looking at factors like:\\n\\nAnswer relevance\\nAnswer accuracy\\nRetrieval quality\\n\\n\\n\\nFor this tutorial, we\\'ll create and evaluate a bot that answers questions about a few of Lilian Weng\\'s insightful blog posts.\\nSetup\\u200b\\nEnvironment\\u200b\\nFirst, let\\'s set our environment variables:\\nPythonTypeScriptimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = \"YOUR LANGSMITH API KEY\"os.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI API KEY\"process.env.LANGSMITH_TRACING = \"true\";process.env.LANGSMITH_API_KEY = \"YOUR LANGSMITH API KEY\";process.env.OPENAI_API_KEY = \"YOUR OPENAI API KEY\";\\nAnd install the dependencies we\\'ll need:\\nPythonTypeScriptpip install -U langsmith langchain[openai] langchain-communityyarn add langsmith langchain @langchain/community @langchain/openai\\nApplication\\u200b\\nFramework FlexibilityWhile this tutorial uses LangChain, the evaluation techniques and LangSmith functionality demonstrated here work with any framework. Feel free to use your preferred tools and libraries.\\nIn this section, we\\'ll build a basic Retrieval-Augmented Generation (RAG) application.\\nWe\\'ll stick to a simple implementation that:\\n\\nIndexing: chunks and indexes a few of Lilian Weng\\'s blogs in a vector store\\nRetrieval: retrieves those chunks based on the user question\\nGeneration: passes the question and retrieved docs to an LLM.\\n\\nIndexing and retrieval\\u200b\\nFirst, lets load the blog posts we want to build a chatbot for and index them.\\nPythonTypeScriptfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitter# List of URLs to load documents fromurls = [    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",]# Load documents from the URLsdocs = [WebBaseLoader(url).load() for url in urls]docs_list = [item for sublist in docs for item in sublist]# Initialize a text splitter with specified chunk size and overlaptext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(    chunk_size=250, chunk_overlap=0)# Split the documents into chunksdoc_splits = text_splitter.split_documents(docs_list)# Add the document chunks to the \"vector store\" using OpenAIEmbeddingsvectorstore = InMemoryVectorStore.from_documents(    documents=doc_splits,    embedding=OpenAIEmbeddings(),)# With langchain we can easily turn any vector store into a retrieval component:retriever = vectorstore.as_retriever(k=6)import { OpenAIEmbeddings } from \"@langchain/openai\";import { MemoryVectorStore } from \"langchain/vectorstores/memory\";import { BrowserbaseLoader } from \"@langchain/community/document_loaders/web/browserbase\";// List of URLs to load documents fromconst urls = [    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",]const loader = new BrowserbaseLoader(urls, {    textContent: true,});const docs = await loader.load();const splitter = new RecursiveCharacterTextSplitter({    chunkSize: 1000, chunkOverlap: 200});const allSplits = await splitter.splitDocuments(docs);const embeddings = new OpenAIEmbeddings({    model: \"text-embedding-3-large\"});const vectorStore = new MemoryVectorStore(embeddings);  // Index chunksawait vectorStore.addDocuments(allSplits)\\nGeneration\\u200b\\nWe can now define the generative pipeline.\\nPythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langsmith import traceablellm = ChatOpenAI(model=\"gpt-4o\", temperature=1)# Add decorator so this function is traced in LangSmith@traceable()def rag_bot(question: str) -> dict:    # LangChain retriever will be automatically traced    docs = retriever.invoke(question)    docs_string = \"\".join(doc.page_content for doc in docs)    instructions = f\"\"\"You are a helpful assistant who is good at analyzing source information and answering questions.       Use the following source documents to answer the user\\'s questions.       If you don\\'t know the answer, just say that you don\\'t know.       Use three sentences maximum and keep the answer concise.Documents:{docs_string}\"\"\"    # langchain ChatModel will be automatically traced    ai_msg = llm.invoke([            {\"role\": \"system\", \"content\": instructions},            {\"role\": \"user\", \"content\": question},        ],    )    return {\"answer\": ai_msg.content, \"documents\": docs}import { ChatOpenAI } from \"@langchain/openai\";import { traceable } from \"langsmith/traceable\";const llm = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 1,})// Add decorator so this function is traced in LangSmithconst ragBot = traceable(    async (question: string) => {        // LangChain retriever will be automatically traced        const retrievedDocs = await vectorStore.similaritySearch(question);        const docsContent = retrievedDocs.map((doc) => doc.pageContent).join(\"\");                const instructions = `You are a helpful assistant who is good at analyzing source information and answering questions        Use the following source documents to answer the user\\'s questions.        If you don\\'t know the answer, just say that you don\\'t know.        Use three sentences maximum and keep the answer concise.        Documents:        ${docsContent}`;                const aiMsg = await llm.invoke([            {                role: \"system\",                content: instructions            },            {                role: \"user\",                content: question            }        ])                return {\"answer\": aiMsg.content, \"documents\": retrievedDocs}    })\\nDataset\\u200b\\nNow that we\\'ve got our application, let\\'s build a dataset to evaluate it. Our dataset will be very simple in this case: we\\'ll have example questions and reference answers.\\nPythonTypeScriptfrom langsmith import Clientclient = Client()# Define the examples for the datasetexamples = [    {        \"inputs\": {\"question\": \"How does the ReAct agent use self-reflection? \"},        \"outputs\": {\"answer\": \"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool outputs.\"},    },    {        \"inputs\": {\"question\": \"What are the types of biases that can arise with few-shot prompting?\"},        \"outputs\": {\"answer\": \"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\"},    },    {        \"inputs\": {\"question\": \"What are five types of adversarial attacks?\"},        \"outputs\": {\"answer\": \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\"},    }]# Create the dataset and examples in LangSmithdataset_name = \"Lilian Weng Blogs Q&A\"dataset = client.create_dataset(dataset_name=dataset_name)client.create_examples(    dataset_id=dataset.id,    examples=examples)import { Client } from \"langsmith\";const client = new Client();// Define the examples for the datasetconst examples = [    [        \"How does the ReAct agent use self-reflection? \",        \"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool outputs.\",    ],    [        \"What are the types of biases that can arise with few-shot prompting?\",        \"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\",    ],    [        \"What are five types of adversarial attacks?\",        \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\",    ]]const [inputs, outputs] = examples.reduce<[Array<{ input: string }>, Array<{ outputs: string }>]>(    ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],    ],    [[], []]);const datasetName = \"Lilian Weng Blogs Q&A\";const dataset = await client.createDataset(datasetName);await client.createExamples({ inputs, outputs, datasetId: dataset.id })\\nEvaluators\\u200b\\nOne way to think about different types of RAG evaluators is as a tuple of what is being evaluated X what its being evaluated against:\\n\\nCorrectness: Response vs reference answer\\n\\n\\nGoal: Measure \"how similar/correct is the RAG chain answer, relative to a ground-truth answer\"\\nMode: Requires a ground truth (reference) answer supplied through a dataset\\nEvaluator: Use LLM-as-judge to assess answer correctness.\\n\\n\\nRelevance: Response vs input\\n\\n\\nGoal: Measure \"how well does the generated response address the initial user input\"\\nMode: Does not require reference answer, because it will compare the answer to the input question\\nEvaluator: Use LLM-as-judge to assess answer relevance, helpfulness, etc.\\n\\n\\nGroundedness: Response vs retrieved docs\\n\\n\\nGoal: Measure \"to what extent does the generated response agree with the retrieved context\"\\nMode: Does not require reference answer, because it will compare the answer to the retrieved context\\nEvaluator: Use LLM-as-judge to assess faithfulness, hallucinations, etc.\\n\\n\\nRetrieval relevance: Retrieved docs vs input\\n\\n\\nGoal: Measure \"how relevant are my retrieved results for this query\"\\nMode: Does not require reference answer, because it will compare the question to the retrieved context\\nEvaluator: Use LLM-as-judge to assess relevance\\n\\n\\nCorrectness: Response vs reference answer\\u200b\\nPythonTypeScriptfrom typing_extensions import Annotated, TypedDict# Grade output schemaclass CorrectnessGrade(TypedDict):    # Note that the order in the fields are defined is the order in which the model will generate them.    # It is useful to put explanations before responses because it forces the model to think through    # its final response before generating it:    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    correct: Annotated[bool, ..., \"True if the answer is correct, False otherwise.\"]# Grade promptcorrectness_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the grade criteria to follow:(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. (2) Ensure that the student answer does not contain any conflicting statements.(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.Correctness:A correctness value of True means that the student\\'s answer meets all of the criteria.A correctness value of False means that the student\\'s answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"# Grader LLMgrader_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(CorrectnessGrade, method=\"json_schema\", strict=True)def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:    \"\"\"An evaluator for RAG answer accuracy\"\"\"    answers = f\"\"\"\\\\QUESTION: {inputs[\\'question\\']}GROUND TRUTH ANSWER: {reference_outputs[\\'answer\\']}STUDENT ANSWER: {outputs[\\'answer\\']}\"\"\"    # Run evaluator    grade = grader_llm.invoke([        {\"role\": \"system\", \"content\": correctness_instructions},         {\"role\": \"user\", \"content\": answers}    ])    return grade[\"correct\"]import type { EvaluationResult } from \"langsmith/evaluation\";import { z } from \"zod\";// Grade promptconst correctnessInstructions = `You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the grade criteria to follow:(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. (2) Ensure that the student answer does not contain any conflicting statements.(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.Correctness:A correctness value of True means that the student\\'s answer meets all of the criteria.A correctness value of False means that the student\\'s answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const graderLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),      correct: z        .boolean()        .describe(\"True if the answer is correct, False otherwise.\")    })    .describe(\"Correctness score for reference answer v.s. generated answer.\"));async function correctness({  inputs,  outputs,  referenceOutputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;  referenceOutputs?: Record<string, any>;}): Promise<EvaluationResult> => {  const answer = `QUESTION: ${inputs.question}    GROUND TRUTH ANSWER: ${reference_outputs.answer}    STUDENT ANSWER: ${outputs.answer}`      // Run evaluator  const grade = graderLLM.invoke([{role: \"system\", content: correctnessInstructions}, {role: \"user\", content: answer}])  return grade.score};\\nRelevance: Response vs input\\u200b\\nThe flow is similar to above, but we simply look at the inputs and outputs without needing the reference_outputs.\\nWithout a reference answer we can\\'t grade accuracy, but can still grade relevance—as in, did the model address the user\\'s question or not.\\nPythonTypeScript# Grade output schemaclass RelevanceGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    relevant: Annotated[bool, ..., \"Provide the score on whether the answer addresses the question\"]# Grade promptrelevance_instructions=\"\"\"You are a teacher grading a quiz. You will be given a QUESTION and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION(2) Ensure the STUDENT ANSWER helps to answer the QUESTIONRelevance:A relevance value of True means that the student\\'s answer meets all of the criteria.A relevance value of False means that the student\\'s answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"# Grader LLMrelevance_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(RelevanceGrade, method=\"json_schema\", strict=True)# Evaluatordef relevance(inputs: dict, outputs: dict) -> bool:    \"\"\"A simple evaluator for RAG answer helpfulness.\"\"\"    answer = f\"QUESTION: {inputs[\\'question\\']}\\\\nSTUDENT ANSWER: {outputs[\\'answer\\']}\"    grade = relevance_llm.invoke([        {\"role\": \"system\", \"content\": relevance_instructions},         {\"role\": \"user\", \"content\": answer}    ])    return grade[\"relevant\"]import type { EvaluationResult } from \"langsmith/evaluation\";import { z } from \"zod\";// Grade promptconst relevanceInstructions = `You are a teacher grading a quiz. You will be given a QUESTION and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION(2) Ensure the STUDENT ANSWER helps to answer the QUESTIONRelevance:A relevance value of True means that the student\\'s answer meets all of the criteria.A relevance value of False means that the student\\'s answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const relevanceLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),      relevant: z        .boolean()        .describe(\"Provide the score on whether the answer addresses the question\")    })    .describe(\"Relevance score for gene\"));async function relevance({  inputs,  outputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;}): Promise<EvaluationResult> => {  const answer = `QUESTION: ${inputs.question}STUDENT ANSWER: ${outputs.answer}`      // Run evaluator  const grade = relevanceLLM.invoke([{role: \"system\", content: relevanceInstructions}, {role: \"user\", content: answer}])  return grade.relevant};\\nGroundedness: Response vs retrieved docs\\u200b\\nAnother useful way to evaluate responses without needing reference answers is to check if the response is justified by (or \"grounded in\") the retrieved documents.\\nPythonTypeScript# Grade output schemaclass GroundedGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    grounded: Annotated[bool, ..., \"Provide the score on if the answer hallucinates from the documents\"]# Grade promptgrounded_instructions = \"\"\"You are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is grounded in the FACTS. (2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.Grounded:A grounded value of True means that the student\\'s answer meets all of the criteria.A grounded value of False means that the student\\'s answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"# Grader LLM grounded_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(GroundedGrade, method=\"json_schema\", strict=True)# Evaluatordef groundedness(inputs: dict, outputs: dict) -> bool:    \"\"\"A simple evaluator for RAG answer groundedness.\"\"\"    doc_string = \"\\\\n\\\\n\".join(doc.page_content for doc in outputs[\"documents\"])    answer = f\"FACTS: {doc_string}\\\\nSTUDENT ANSWER: {outputs[\\'answer\\']}\"    grade = grounded_llm.invoke([{\"role\": \"system\", \"content\": grounded_instructions}, {\"role\": \"user\", \"content\": answer}])    return grade[\"grounded\"]import type { EvaluationResult } from \"langsmith/evaluation\";import { z } from \"zod\";// Grade promptconst groundedInstructions = `You are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is grounded in the FACTS. (2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.Grounded:A grounded value of True means that the student\\'s answer meets all of the criteria.A grounded value of False means that the student\\'s answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const groundedLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),      grounded: z        .boolean()        .describe(\"Provide the score on if the answer hallucinates from the documents\")    })    .describe(\"Grounded score for the answer from the retrieved documents.\"));async function grounded({  inputs,  outputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;}): Promise<EvaluationResult> => {  const docString =  outputs.documents.map((doc) => doc.pageContent).join(\"\");  const answer = `FACTS: ${docString}    STUDENT ANSWER: ${outputs.answer}`      // Run evaluator  const grade = groundedLLM.invoke([{role: \"system\", content: groundedInstructions}, {role: \"user\", content: answer}])  return grade.grounded};\\nRetrieval relevance: Retrieved docs vs input\\u200b\\nPythonTypeScript# Grade output schemaclass RetrievalRelevanceGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    relevant: Annotated[bool, ..., \"True if the retrieved documents are relevant to the question, False otherwise\"]# Grade promptretrieval_relevance_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION and a set of FACTS provided by the student. Here is the grade criteria to follow:(1) You goal is to identify FACTS that are completely unrelated to the QUESTION(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is metRelevance:A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.A relevance value of False means that the FACTS are completely unrelated to the QUESTION.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"# Grader LLMretrieval_relevance_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(RetrievalRelevanceGrade, method=\"json_schema\", strict=True)def retrieval_relevance(inputs: dict, outputs: dict) -> bool:    \"\"\"An evaluator for document relevance\"\"\"    doc_string = \"\\\\n\\\\n\".join(doc.page_content for doc in outputs[\"documents\"])    answer = f\"FACTS: {doc_string}\\\\nQUESTION: {inputs[\\'question\\']}\"    # Run evaluator    grade = retrieval_relevance_llm.invoke([        {\"role\": \"system\", \"content\": retrieval_relevance_instructions},         {\"role\": \"user\", \"content\": answer}    ])    return grade[\"relevant\"]import type { EvaluationResult } from \"langsmith/evaluation\";import { z } from \"zod\";// Grade promptconst retrievalRelevanceInstructions = `You are a teacher grading a quiz. You will be given a QUESTION and a set of FACTS provided by the student. Here is the grade criteria to follow:(1) You goal is to identify FACTS that are completely unrelated to the QUESTION(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is metRelevance:A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.A relevance value of False means that the FACTS are completely unrelated to the QUESTION.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const retrievalRelevanceLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),      relevant: z        .boolean()        .describe(\"True if the retrieved documents are relevant to the question, False otherwise\")    })    .describe(\"Retrieval relevance score for the retrieved documents v.s. the question.\"));async function retrievalRelevance({  inputs,  outputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;}): Promise<EvaluationResult> => {  const docString =  outputs.documents.map((doc) => doc.pageContent).join(\"\");  const answer = `FACTS: ${docString}    QUESTION: ${inputs.question}`      // Run evaluator  const grade = retrievalRelevanceLLM.invoke([{role: \"system\", content: retrievalRelevanceInstructions}, {role: \"user\", content: answer}])  return grade.relevant};\\nRun evaluation\\u200b\\nWe can now kick off our evaluation job with all of our different evaluators.\\nPythonTypeScriptdef target(inputs: dict) -> dict:    return rag_bot(inputs[\"question\"])experiment_results = client.evaluate(    target,    data=dataset_name,    evaluators=[correctness, groundedness, relevance, retrieval_relevance],    experiment_prefix=\"rag-doc-relevance\",    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},)# Explore results locally as a dataframe if you have pandas installed# experiment_results.to_pandas()import { evaluate } from \"langsmith/evaluation\";const targetFunc = (inputs: Record<string, any>) => {    return ragBot(inputs.question)};const experimentResults = await evaluate(targetFunc, {    data: datasetName,    evaluators: [correctness, groundedness, relevance, retrievalRelevance],    experimentPrefix=\"rag-doc-relevance\",    metadata={version: \"LCEL context, gpt-4-0125-preview\"},});\\nYou can see an example of what these results look like here: LangSmith link\\nReference code\\u200b\\nHere\\'s a consolidated script with all the above code:PythonTypeScriptfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langsmith import Client, traceablefrom typing_extensions import Annotated, TypedDict# List of URLs to load documents fromurls = [    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",]# Load documents from the URLsdocs = [WebBaseLoader(url).load() for url in urls]docs_list = [item for sublist in docs for item in sublist]# Initialize a text splitter with specified chunk size and overlaptext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(    chunk_size=250, chunk_overlap=0)# Split the documents into chunksdoc_splits = text_splitter.split_documents(docs_list)# Add the document chunks to the \"vector store\" using OpenAIEmbeddingsvectorstore = InMemoryVectorStore.from_documents(    documents=doc_splits,    embedding=OpenAIEmbeddings(),)# With langchain we can easily turn any vector store into a retrieval component:retriever = vectorstore.as_retriever(k=6)llm = ChatOpenAI(model=\"gpt-4o\", temperature=1)# Add decorator so this function is traced in LangSmith@traceable()def rag_bot(question: str) -> dict:    # langchain Retriever will be automatically traced    docs = retriever.invoke(question)    docs_string = \"\".join(doc.page_content for doc in docs)    instructions = f\"\"\"You are a helpful assistant who is good at analyzing source information and answering questions.       Use the following source documents to answer the user\\'s questions.       If you don\\'t know the answer, just say that you don\\'t know.       Use three sentences maximum and keep the answer concise.Documents:{docs_string}\"\"\"    # langchain ChatModel will be automatically traced    ai_msg = llm.invoke(        [            {\"role\": \"system\", \"content\": instructions},            {\"role\": \"user\", \"content\": question},        ],    )    return {\"answer\": ai_msg.content, \"documents\": docs}client = Client()# Define the examples for the datasetexamples = [    {        \"inputs\": {\"question\": \"How does the ReAct agent use self-reflection? \"},        \"outputs\": {\"answer\": \"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool outputs.\"},    },    {        \"inputs\": {\"question\": \"What are the types of biases that can arise with few-shot prompting?\"},        \"outputs\": {\"answer\": \"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\"},    },    {        \"inputs\": {\"question\": \"What are five types of adversarial attacks?\"},        \"outputs\": {\"answer\": \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\"},    },]# Create the dataset and examples in LangSmithdataset_name = \"Lilian Weng Blogs Q&A\"if not client.has_dataset(dataset_name=dataset_name):    dataset = client.create_dataset(dataset_name=dataset_name)    client.create_examples(        dataset_id=dataset.id,        examples=examples    )# Grade output schemaclass CorrectnessGrade(TypedDict):    # Note that the order in the fields are defined is the order in which the model will generate them.    # It is useful to put explanations before responses because it forces the model to think through    # its final response before generating it:    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    correct: Annotated[bool, ..., \"True if the answer is correct, False otherwise.\"]# Grade promptcorrectness_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the grade criteria to follow:(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. (2) Ensure that the student answer does not contain any conflicting statements.(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.Correctness:A correctness value of True means that the student\\'s answer meets all of the criteria.A correctness value of False means that the student\\'s answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"# Grader LLMgrader_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(    CorrectnessGrade, method=\"json_schema\", strict=True)def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:    \"\"\"An evaluator for RAG answer accuracy\"\"\"    answers = f\"\"\"\\\\QUESTION: {inputs[\\'question\\']}GROUND TRUTH ANSWER: {reference_outputs[\\'answer\\']}STUDENT ANSWER: {outputs[\\'answer\\']}\"\"\"    # Run evaluator    grade = grader_llm.invoke(        [            {\"role\": \"system\", \"content\": correctness_instructions},            {\"role\": \"user\", \"content\": answers},        ]    )    return grade[\"correct\"]# Grade output schemaclass RelevanceGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    relevant: Annotated[        bool, ..., \"Provide the score on whether the answer addresses the question\"    ]# Grade promptrelevance_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION(2) Ensure the STUDENT ANSWER helps to answer the QUESTIONRelevance:A relevance value of True means that the student\\'s answer meets all of the criteria.A relevance value of False means that the student\\'s answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"# Grader LLMrelevance_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(    RelevanceGrade, method=\"json_schema\", strict=True)# Evaluatordef relevance(inputs: dict, outputs: dict) -> bool:    \"\"\"A simple evaluator for RAG answer helpfulness.\"\"\"    answer = f\"QUESTION: {inputs[\\'question\\']}\\\\nSTUDENT ANSWER: {outputs[\\'answer\\']}\"    grade = relevance_llm.invoke(        [            {\"role\": \"system\", \"content\": relevance_instructions},            {\"role\": \"user\", \"content\": answer},        ]    )    return grade[\"relevant\"]# Grade output schemaclass GroundedGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    grounded: Annotated[        bool, ..., \"Provide the score on if the answer hallucinates from the documents\"    ]# Grade promptgrounded_instructions = \"\"\"You are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is grounded in the FACTS. (2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.Grounded:A grounded value of True means that the student\\'s answer meets all of the criteria.A grounded value of False means that the student\\'s answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"# Grader LLMgrounded_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(    GroundedGrade, method=\"json_schema\", strict=True)# Evaluatordef groundedness(inputs: dict, outputs: dict) -> bool:    \"\"\"A simple evaluator for RAG answer groundedness.\"\"\"    doc_string = \"\\\\n\\\\n\".join(doc.page_content for doc in outputs[\"documents\"])    answer = f\"FACTS: {doc_string}\\\\nSTUDENT ANSWER: {outputs[\\'answer\\']}\"    grade = grounded_llm.invoke(        [            {\"role\": \"system\", \"content\": grounded_instructions},            {\"role\": \"user\", \"content\": answer},        ]    )    return grade[\"grounded\"]# Grade output schemaclass RetrievalRelevanceGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    relevant: Annotated[        bool,        ...,        \"True if the retrieved documents are relevant to the question, False otherwise\",    ]# Grade promptretrieval_relevance_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION and a set of FACTS provided by the student. Here is the grade criteria to follow:(1) You goal is to identify FACTS that are completely unrelated to the QUESTION(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is metRelevance:A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.A relevance value of False means that the FACTS are completely unrelated to the QUESTION.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"# Grader LLMretrieval_relevance_llm = ChatOpenAI(    model=\"gpt-4o\", temperature=0).with_structured_output(RetrievalRelevanceGrade, method=\"json_schema\", strict=True)def retrieval_relevance(inputs: dict, outputs: dict) -> bool:    \"\"\"An evaluator for document relevance\"\"\"    doc_string = \"\\\\n\\\\n\".join(doc.page_content for doc in outputs[\"documents\"])    answer = f\"FACTS: {doc_string}\\\\nQUESTION: {inputs[\\'question\\']}\"    # Run evaluator    grade = retrieval_relevance_llm.invoke(        [            {\"role\": \"system\", \"content\": retrieval_relevance_instructions},            {\"role\": \"user\", \"content\": answer},        ]    )    return grade[\"relevant\"]def target(inputs: dict) -> dict:    return rag_bot(inputs[\"question\"])experiment_results = client.evaluate(    target,    data=dataset_name,    evaluators=[correctness, groundedness, relevance, retrieval_relevance],    experiment_prefix=\"rag-doc-relevance\",    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},)# Explore results locally as a dataframe if you have pandas installed# experiment_results.to_pandas()import { OpenAIEmbeddings, ChatOpenAI } from \"@langchain/openai\";import { MemoryVectorStore } from \"langchain/vectorstores/memory\";import { BrowserbaseLoader } from \"@langchain/community/document_loaders/web/browserbase\";import { traceable } from \"langsmith/traceable\";import { Client } from \"langsmith\";import { evaluate, type EvaluationResult } from \"langsmith/evaluation\";import { z } from \"zod\";// List of URLs to load documents fromconst urls = [    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",]const loader = new BrowserbaseLoader(urls, {    textContent: true,});const docs = await loader.load();const splitter = new RecursiveCharacterTextSplitter({    chunkSize: 1000, chunkOverlap: 200});const allSplits = await splitter.splitDocuments(docs);const embeddings = new OpenAIEmbeddings({    model: \"text-embedding-3-large\"});const vectorStore = new MemoryVectorStore(embeddings);  // Index chunksawait vectorStore.addDocuments(allSplits)   const llm = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 1,})// Add decorator so this function is traced in LangSmithconst ragBot = traceable(    async (question: string) => {        // LangChain retriever will be automatically traced        const retrievedDocs = await vectorStore.similaritySearch(question);        const docsContent = retrievedDocs.map((doc) => doc.pageContent).join(\"\");                const instructions = `You are a helpful assistant who is good at analyzing source information and answering questions.        Use the following source documents to answer the user\\'s questions.        If you don\\'t know the answer, just say that you don\\'t know.        Use three sentences maximum and keep the answer concise.        Documents:        ${docsContent}`                const aiMsg = await llm.invoke([            {                role: \"system\",                content: instructions            },            {                role: \"user\",                content: question            }        ])                return {\"answer\": aiMsg.content, \"documents\": retrievedDocs}    })const client = new Client();// Define the examples for the datasetconst examples = [    [        \"How does the ReAct agent use self-reflection? \",        \"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool outputs.\",    ],    [        \"What are the types of biases that can arise with few-shot prompting?\",        \"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\",    ],    [        \"What are five types of adversarial attacks?\",        \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\",    ]]const [inputs, outputs] = examples.reduce<[Array<{ input: string }>, Array<{ outputs: string }>]>(    ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],    ],    [[], []]);const datasetName = \"Lilian Weng Blogs Q&A\";const dataset = await client.createDataset(datasetName);await client.createExamples({ inputs, outputs, datasetId: dataset.id })// Grade promptconst correctnessInstructions = `You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the grade criteria to follow:(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. (2) Ensure that the student answer does not contain any conflicting statements.(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.Correctness:A correctness value of True means that the student\\'s answer meets all of the criteria.A correctness value of False means that the student\\'s answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const graderLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),      correct: z        .boolean()        .describe(\"True if the answer is correct, False otherwise.\")    })    .describe(\"Correctness score for reference answer v.s. generated answer.\"));async function correctness({  inputs,  outputs,  referenceOutputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;  referenceOutputs?: Record<string, any>;}): Promise<EvaluationResult> => {  const answer = `QUESTION: ${inputs.question}    GROUND TRUTH ANSWER: ${reference_outputs.answer}    STUDENT ANSWER: ${outputs.answer}`      // Run evaluator  const grade = graderLLM.invoke([{role: \"system\", content: correctnessInstructions}, {role: \"user\", content: answer}])  return grade.score};// Grade promptconst relevanceInstructions = `You are a teacher grading a quiz. You will be given a QUESTION and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION(2) Ensure the STUDENT ANSWER helps to answer the QUESTIONRelevance:A relevance value of True means that the student\\'s answer meets all of the criteria.A relevance value of False means that the student\\'s answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const relevanceLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),      relevant: z        .boolean()        .describe(\"Provide the score on whether the answer addresses the question\")    })    .describe(\"Relevance score for gene\"));async function relevance({  inputs,  outputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;}): Promise<EvaluationResult> => {  const answer = `QUESTION: ${inputs.question}STUDENT ANSWER: ${outputs.answer}`      // Run evaluator  const grade = relevanceLLM.invoke([{role: \"system\", content: relevanceInstructions}, {role: \"user\", content: answer}])  return grade.relevant};// Grade promptconst groundedInstructions = `You are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is grounded in the FACTS. (2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.Grounded:A grounded value of True means that the student\\'s answer meets all of the criteria.A grounded value of False means that the student\\'s answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const groundedLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),      grounded: z        .boolean()        .describe(\"Provide the score on if the answer hallucinates from the documents\")    })    .describe(\"Grounded score for the answer from the retrieved documents.\"));async function grounded({  inputs,  outputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;}): Promise<EvaluationResult> => {  const docString =  outputs.documents.map((doc) => doc.pageContent).join(\"\");  const answer = `FACTS: ${docString}    STUDENT ANSWER: ${outputs.answer}`      // Run evaluator  const grade = groundedLLM.invoke([{role: \"system\", content: groundedInstructions}, {role: \"user\", content: answer}])  return grade.grounded};// Grade promptconst retrievalRelevanceInstructions = `You are a teacher grading a quiz. You will be given a QUESTION and a set of FACTS provided by the student. Here is the grade criteria to follow:(1) You goal is to identify FACTS that are completely unrelated to the QUESTION(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is metRelevance:A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.A relevance value of False means that the FACTS are completely unrelated to the QUESTION.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const retrievalRelevanceLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),      relevant: z        .boolean()        .describe(\"True if the retrieved documents are relevant to the question, False otherwise\")    })    .describe(\"Retrieval relevance score for the retrieved documents v.s. the question.\"));async function retrievalRelevance({  inputs,  outputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;}): Promise<EvaluationResult> => {  const docString =  outputs.documents.map((doc) => doc.pageContent).join(\"\");  const answer = `FACTS: ${docString}    QUESTION: ${inputs.question}`      // Run evaluator  const grade = retrievalRelevanceLLM.invoke([{role: \"system\", content: retrievalRelevanceInstructions}, {role: \"user\", content: answer}])  return grade.relevant};const targetFunc = (input: Record<string, any>) => {    return ragBot(inputs.question)};const experimentResults = await evaluate(targetFunc, {    data: datasetName,    evaluators: [correctness, groundedness, relevance, retrievalRelevance],    experimentPrefix=\"rag-doc-relevance\",    metadata={version: \"LCEL context, gpt-4-0125-preview\"},});Was this page helpful?You can leave detailed feedback on GitHub.PreviousEvaluate a chatbotNextRun backtests on a new version of an agentOverviewSetupEnvironmentApplicationDatasetEvaluatorsCorrectness: Response vs reference answerRelevance: Response vs inputGroundedness: Response vs retrieved docsRetrieval relevance: Retrieved docs vs inputRun evaluationReference codeCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "295830ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Evaluate a RAG application | 🦜️🛠️ LangSmith'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='to AppGet StartedObservabilityEvaluationQuick StartTutorialsEvaluate a chatbotEvaluate a RAG applicationRun backtests on a new version of an agentRunning SWE-bench with LangSmithEvaluate a complex'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='LangSmithEvaluate a complex agentImproving LLM-as-judge evaluators using human feedbackTest a ReAct agent with Pytest/Vitest and LangSmithHow-to GuidesAnalyze a single experimentLog user feedback'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='experimentLog user feedback using the SDKHow to run an evaluationCreating and Managing Datasets in the UIRenaming an experimentAutomatically run evaluators on experimentsHow to manage datasets'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"to manage datasets programmaticallyRunning an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate an application's intermediate stepsHow to\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='intermediate stepsHow to version a datasetUse annotation queuesDataset SharingHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to run evals with pytest (beta)Run pairwise evaluationsHow to'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='pairwise evaluationsHow to audit evaluator scoresHow to improve your evaluator with few-shot examplesHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to define a target function to evaluateHow to download experiment results as a CSVRun an evaluation with'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='a CSVRun an evaluation with multimodal contentHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='evaluatorHow to run an evaluation locally (Python only)How to return categorical vs numerical metricsHow to simulate multi-turn interactionsHow to return multiple scores in one evaluatorHow to use'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='in one evaluatorHow to use prebuilt evaluatorsHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorHow to run evals with Vitest/Jest (beta)Conceptual'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Vitest/Jest (beta)Conceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationTutorialsEvaluate a RAG applicationOn this pageEvaluate a RAG application'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Key conceptsRAG evaluation | Evaluators | LLM-as-judge evaluators'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Retrieval Augmented Generation (RAG) is a technique that enhances Large Language Models (LLMs) by providing them with relevant external knowledge. It has become one of the most widely used approaches'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='most widely used approaches for building LLM applications.'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"This tutorial will show you how to evaluate your RAG applications using LangSmith. You'll learn:\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"How to create test datasets\\nHow to run your RAG application on those datasets\\nHow to measure your application's performance using different evaluation metrics\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Overview\\u200b\\nA typical RAG evaluation workflow consists of three main steps:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Creating a dataset with questions and their expected answers\\nRunning your RAG application on those questions\\nUsing evaluators to measure how well your application performed, looking at factors like:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Answer relevance\\nAnswer accuracy\\nRetrieval quality'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"For this tutorial, we'll create and evaluate a bot that answers questions about a few of Lilian Weng's insightful blog posts.\\nSetup\\u200b\\nEnvironment\\u200b\\nFirst, let's set our environment variables:\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScriptimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = \"YOUR LANGSMITH API KEY\"os.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI API'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= \"YOUR OPENAI API KEY\"process.env.LANGSMITH_TRACING = \"true\";process.env.LANGSMITH_API_KEY = \"YOUR LANGSMITH API KEY\";process.env.OPENAI_API_KEY = \"YOUR OPENAI API KEY\";'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"And install the dependencies we'll need:\\nPythonTypeScriptpip install -U langsmith langchain[openai] langchain-communityyarn add langsmith langchain @langchain/community @langchain/openai\\nApplication\\u200b\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Framework FlexibilityWhile this tutorial uses LangChain, the evaluation techniques and LangSmith functionality demonstrated here work with any framework. Feel free to use your preferred tools and'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='use your preferred tools and libraries.'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"In this section, we'll build a basic Retrieval-Augmented Generation (RAG) application.\\nWe'll stick to a simple implementation that:\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"Indexing: chunks and indexes a few of Lilian Weng's blogs in a vector store\\nRetrieval: retrieves those chunks based on the user question\\nGeneration: passes the question and retrieved docs to an LLM.\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Indexing and retrieval\\u200b\\nFirst, lets load the blog posts we want to build a chatbot for and index them.'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScriptfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsfrom'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitter# List of URLs to load documents fromurls = [    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",]# Load documents from the URLsdocs ='), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='documents from the URLsdocs = [WebBaseLoader(url).load() for url in urls]docs_list = [item for sublist in docs for item in sublist]# Initialize a text splitter with specified chunk size and'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='with specified chunk size and overlaptext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(    chunk_size=250, chunk_overlap=0)# Split the documents into chunksdoc_splits ='), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='into chunksdoc_splits = text_splitter.split_documents(docs_list)# Add the document chunks to the \"vector store\" using OpenAIEmbeddingsvectorstore = InMemoryVectorStore.from_documents('), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='documents=doc_splits,    embedding=OpenAIEmbeddings(),)# With langchain we can easily turn any vector store into a retrieval component:retriever = vectorstore.as_retriever(k=6)import {'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{ OpenAIEmbeddings } from \"@langchain/openai\";import { MemoryVectorStore } from \"langchain/vectorstores/memory\";import { BrowserbaseLoader } from'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{ BrowserbaseLoader } from \"@langchain/community/document_loaders/web/browserbase\";// List of URLs to load documents fromconst urls = [    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",]const loader = new BrowserbaseLoader(urls, {'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='BrowserbaseLoader(urls, {    textContent: true,});const docs = await loader.load();const splitter = new RecursiveCharacterTextSplitter({    chunkSize: 1000, chunkOverlap: 200});const allSplits ='), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='200});const allSplits = await splitter.splitDocuments(docs);const embeddings = new OpenAIEmbeddings({    model: \"text-embedding-3-large\"});const vectorStore = new MemoryVectorStore(embeddings);  //'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='// Index chunksawait vectorStore.addDocuments(allSplits)'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Generation\\u200b\\nWe can now define the generative pipeline.'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langsmith import traceablellm = ChatOpenAI(model=\"gpt-4o\", temperature=1)# Add decorator so this function is traced in'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='so this function is traced in LangSmith@traceable()def rag_bot(question: str) -> dict:    # LangChain retriever will be automatically traced    docs = retriever.invoke(question)    docs_string ='), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='docs_string = \"\".join(doc.page_content for doc in docs)    instructions = f\"\"\"You are a helpful assistant who is good at analyzing source information and answering questions.       Use the'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"questions.       Use the following source documents to answer the user's questions.       If you don't know the answer, just say that you don't know.       Use three sentences maximum and keep the\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='maximum and keep the answer concise.Documents:{docs_string}\"\"\"    # langchain ChatModel will be automatically traced    ai_msg = llm.invoke([            {\"role\": \"system\", \"content\": instructions},'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"content\": instructions},            {\"role\": \"user\", \"content\": question},        ],    )    return {\"answer\": ai_msg.content, \"documents\": docs}import { ChatOpenAI } from \"@langchain/openai\";import'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"@langchain/openai\";import { traceable } from \"langsmith/traceable\";const llm = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 1,})// Add decorator so this function is traced in LangSmithconst'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='is traced in LangSmithconst ragBot = traceable(    async (question: string) => {        // LangChain retriever will be automatically traced        const retrievedDocs = await'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='const retrievedDocs = await vectorStore.similaritySearch(question);        const docsContent = retrievedDocs.map((doc) => doc.pageContent).join(\"\");                const instructions = `You are a'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"instructions = `You are a helpful assistant who is good at analyzing source information and answering questions        Use the following source documents to answer the user's questions.        If you\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"questions.        If you don't know the answer, just say that you don't know.        Use three sentences maximum and keep the answer concise.        Documents:        ${docsContent}`;\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='const aiMsg = await llm.invoke([            {                role: \"system\",                content: instructions            },            {                role: \"user\",'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='role: \"user\",                content: question            }        ])                return {\"answer\": aiMsg.content, \"documents\": retrievedDocs}    })'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"Dataset\\u200b\\nNow that we've got our application, let's build a dataset to evaluate it. Our dataset will be very simple in this case: we'll have example questions and reference answers.\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScriptfrom langsmith import Clientclient = Client()# Define the examples for the datasetexamples = [    {        \"inputs\": {\"question\": \"How does the ReAct agent use self-reflection? \"},'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='use self-reflection? \"},        \"outputs\": {\"answer\": \"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='/ reasoning about the tool outputs.\"},    },    {        \"inputs\": {\"question\": \"What are the types of biases that can arise with few-shot prompting?\"},        \"outputs\": {\"answer\": \"The biases that'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{\"answer\": \"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\"},    },    {        \"inputs\": {\"question\": \"What are five'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{\"question\": \"What are five types of adversarial attacks?\"},        \"outputs\": {\"answer\": \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\"},    }]# Create the dataset and examples in LangSmithdataset_name = \"Lilian Weng Blogs Q&A\"dataset ='), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Weng Blogs Q&A\"dataset = client.create_dataset(dataset_name=dataset_name)client.create_examples(    dataset_id=dataset.id,    examples=examples)import { Client } from \"langsmith\";const client = new'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='client = new Client();// Define the examples for the datasetconst examples = [    [        \"How does the ReAct agent use self-reflection? \",        \"ReAct integrates reasoning and acting, performing'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool outputs.\",    ],    [        \"What are the types of biases that can arise with'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='of biases that can arise with few-shot prompting?\",        \"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\",    ],'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Common token bias.\",    ],    [        \"What are five types of adversarial attacks?\",        \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\",    ]]const [inputs, outputs] = examples.reduce<[Array<{ input: string }>, Array<{ outputs: string }>]>('), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='outputs: string }>]>(    ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],    ],    [[], []]);const datasetName = \"Lilian Weng Blogs Q&A\";const'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"Lilian Weng Blogs Q&A\";const dataset = await client.createDataset(datasetName);await client.createExamples({ inputs, outputs, datasetId: dataset.id })'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Evaluators\\u200b\\nOne way to think about different types of RAG evaluators is as a tuple of what is being evaluated X what its being evaluated against:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Correctness: Response vs reference answer'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Goal: Measure \"how similar/correct is the RAG chain answer, relative to a ground-truth answer\"\\nMode: Requires a ground truth (reference) answer supplied through a dataset'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Evaluator: Use LLM-as-judge to assess answer correctness.'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Relevance: Response vs input'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Goal: Measure \"how well does the generated response address the initial user input\"\\nMode: Does not require reference answer, because it will compare the answer to the input question'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Evaluator: Use LLM-as-judge to assess answer relevance, helpfulness, etc.'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Groundedness: Response vs retrieved docs'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Goal: Measure \"to what extent does the generated response agree with the retrieved context\"\\nMode: Does not require reference answer, because it will compare the answer to the retrieved context'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Evaluator: Use LLM-as-judge to assess faithfulness, hallucinations, etc.'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Retrieval relevance: Retrieved docs vs input'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Goal: Measure \"how relevant are my retrieved results for this query\"\\nMode: Does not require reference answer, because it will compare the question to the retrieved context'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Evaluator: Use LLM-as-judge to assess relevance'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Correctness: Response vs reference answer\\u200b'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScriptfrom typing_extensions import Annotated, TypedDict# Grade output schemaclass CorrectnessGrade(TypedDict):    # Note that the order in the fields are defined is the order in which the'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='is the order in which the model will generate them.    # It is useful to put explanations before responses because it forces the model to think through    # its final response before generating it:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='before generating it:    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    correct: Annotated[bool, ..., \"True if the answer is correct, False otherwise.\"]# Grade'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='False otherwise.\"]# Grade promptcorrectness_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='STUDENT ANSWER. Here is the grade criteria to follow:(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. (2) Ensure that the student answer does'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='that the student answer does not contain any conflicting statements.(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"as it is factually accurate relative to the  ground truth answer.Correctness:A correctness value of True means that the student's answer meets all of the criteria.A correctness value of False means\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='are correct. Avoid simply stating the correct answer at the outset.\"\"\"# Grader LLMgrader_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(CorrectnessGrade, method=\"json_schema\",'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='method=\"json_schema\", strict=True)def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:    \"\"\"An evaluator for RAG answer accuracy\"\"\"    answers = f\"\"\"\\\\QUESTION:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='answers = f\"\"\"\\\\QUESTION: {inputs[\\'question\\']}GROUND TRUTH ANSWER: {reference_outputs[\\'answer\\']}STUDENT ANSWER: {outputs[\\'answer\\']}\"\"\"    # Run evaluator    grade = grader_llm.invoke(['), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= grader_llm.invoke([        {\"role\": \"system\", \"content\": correctness_instructions},         {\"role\": \"user\", \"content\": answers}    ])    return grade[\"correct\"]import type { EvaluationResult }'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='type { EvaluationResult } from \"langsmith/evaluation\";import { z } from \"zod\";// Grade promptconst correctnessInstructions = `You are a teacher grading a quiz. You will be given a QUESTION, the'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the grade criteria to follow:(1) Grade the student answers based ONLY on their factual accuracy relative'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='factual accuracy relative to the ground truth answer. (2) Ensure that the student answer does not contain any conflicting statements.(3) It is OK if the student answer contains more information than'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.Correctness:A correctness value of True means that the student's answer meets\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"the student's answer meets all of the criteria.A correctness value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const graderLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),      correct: z        .boolean()'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='z        .boolean()        .describe(\"True if the answer is correct, False otherwise.\")    })    .describe(\"Correctness score for reference answer v.s. generated answer.\"));async function'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='answer.\"));async function correctness({  inputs,  outputs,  referenceOutputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;  referenceOutputs?: Record<string, any>;}):'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Record<string, any>;}): Promise<EvaluationResult> => {  const answer = `QUESTION: ${inputs.question}    GROUND TRUTH ANSWER: ${reference_outputs.answer}    STUDENT ANSWER: ${outputs.answer}`      //'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='${outputs.answer}`      // Run evaluator  const grade = graderLLM.invoke([{role: \"system\", content: correctnessInstructions}, {role: \"user\", content: answer}])  return grade.score};'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Relevance: Response vs input\\u200b\\nThe flow is similar to above, but we simply look at the inputs and outputs without needing the reference_outputs.'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"Without a reference answer we can't grade accuracy, but can still grade relevance—as in, did the model address the user's question or not.\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScript# Grade output schemaclass RelevanceGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    relevant: Annotated[bool, ..., \"Provide the score'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='..., \"Provide the score on whether the answer addresses the question\"]# Grade promptrelevance_instructions=\"\"\"You are a teacher grading a quiz. You will be given a QUESTION and a STUDENT ANSWER. Here'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION(2) Ensure the STUDENT ANSWER helps to answer the QUESTIONRelevance:A'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"the QUESTIONRelevance:A relevance value of True means that the student's answer meets all of the criteria.A relevance value of False means that the student's answer does not meet all of the\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"#'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='answer at the outset.\"\"\"# Grader LLMrelevance_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(RelevanceGrade, method=\"json_schema\", strict=True)# Evaluatordef relevance(inputs:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='relevance(inputs: dict, outputs: dict) -> bool:    \"\"\"A simple evaluator for RAG answer helpfulness.\"\"\"    answer = f\"QUESTION: {inputs[\\'question\\']}\\\\nSTUDENT ANSWER: {outputs[\\'answer\\']}\"    grade ='), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='grade = relevance_llm.invoke([        {\"role\": \"system\", \"content\": relevance_instructions},         {\"role\": \"user\", \"content\": answer}    ])    return grade[\"relevant\"]import type {'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='type { EvaluationResult } from \"langsmith/evaluation\";import { z } from \"zod\";// Grade promptconst relevanceInstructions = `You are a teacher grading a quiz. You will be given a QUESTION and a'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='be given a QUESTION and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION(2) Ensure the STUDENT ANSWER helps to answer the'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"ANSWER helps to answer the QUESTIONRelevance:A relevance value of True means that the student's answer meets all of the criteria.A relevance value of False means that the student's answer does not\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='stating the correct answer at the outset.`const relevanceLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='z        .string()        .describe(\"Explain your reasoning for the score\"),      relevant: z        .boolean()        .describe(\"Provide the score on whether the answer addresses the question\")'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='addresses the question\")    })    .describe(\"Relevance score for gene\"));async function relevance({  inputs,  outputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;}):'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Record<string, any>;}): Promise<EvaluationResult> => {  const answer = `QUESTION: ${inputs.question}STUDENT ANSWER: ${outputs.answer}`      // Run evaluator  const grade = relevanceLLM.invoke([{role:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= relevanceLLM.invoke([{role: \"system\", content: relevanceInstructions}, {role: \"user\", content: answer}])  return grade.relevant};'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Groundedness: Response vs retrieved docs\\u200b'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Another useful way to evaluate responses without needing reference answers is to check if the response is justified by (or \"grounded in\") the retrieved documents.'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScript# Grade output schemaclass GroundedGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    grounded: Annotated[bool, ..., \"Provide the score'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='..., \"Provide the score on if the answer hallucinates from the documents\"]# Grade promptgrounded_instructions = \"\"\"You are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is grounded in the FACTS. (2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"information outside the scope of the FACTS.Grounded:A grounded value of True means that the student's answer meets all of the criteria.A grounded value of False means that the student's answer does\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='stating the correct answer at the outset.\"\"\"# Grader LLM grounded_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(GroundedGrade, method=\"json_schema\", strict=True)#'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='strict=True)# Evaluatordef groundedness(inputs: dict, outputs: dict) -> bool:    \"\"\"A simple evaluator for RAG answer groundedness.\"\"\"    doc_string = \"\\\\n\\\\n\".join(doc.page_content for doc in'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='for doc in outputs[\"documents\"])    answer = f\"FACTS: {doc_string}\\\\nSTUDENT ANSWER: {outputs[\\'answer\\']}\"    grade = grounded_llm.invoke([{\"role\": \"system\", \"content\": grounded_instructions}, {\"role\":'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{\"role\": \"user\", \"content\": answer}])    return grade[\"grounded\"]import type { EvaluationResult } from \"langsmith/evaluation\";import { z } from \"zod\";// Grade promptconst groundedInstructions = `You'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='groundedInstructions = `You are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is grounded in the FACTS.'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='is grounded in the FACTS. (2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.Grounded:A grounded value of True means that the student\\'s answer'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"that the student's answer meets all of the criteria.A grounded value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const groundedLLM = new ChatOpenAI({  model: \"gpt-4o\",'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),      grounded: z'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='grounded: z        .boolean()        .describe(\"Provide the score on if the answer hallucinates from the documents\")    })    .describe(\"Grounded score for the answer from the retrieved'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='the answer from the retrieved documents.\"));async function grounded({  inputs,  outputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;}): Promise<EvaluationResult> => {  const'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='=> {  const docString =  outputs.documents.map((doc) => doc.pageContent).join(\"\");  const answer = `FACTS: ${docString}    STUDENT ANSWER: ${outputs.answer}`      // Run evaluator  const grade ='), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Run evaluator  const grade = groundedLLM.invoke([{role: \"system\", content: groundedInstructions}, {role: \"user\", content: answer}])  return grade.grounded};'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Retrieval relevance: Retrieved docs vs input\\u200b'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScript# Grade output schemaclass RetrievalRelevanceGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    relevant: Annotated[bool, ..., \"True if'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Annotated[bool, ..., \"True if the retrieved documents are relevant to the question, False otherwise\"]# Grade promptretrieval_relevance_instructions = \"\"\"You are a teacher grading a quiz. You will be'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='grading a quiz. You will be given a QUESTION and a set of FACTS provided by the student. Here is the grade criteria to follow:(1) You goal is to identify FACTS that are completely unrelated to the'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='completely unrelated to the QUESTION(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant(3) It is OK if the facts have SOME information that is'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='have SOME information that is unrelated to the question as long as (2) is metRelevance:A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='related to the QUESTION and are therefore relevant.A relevance value of False means that the FACTS are completely unrelated to the QUESTION.Explain your reasoning in a step-by-step manner to ensure'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"# Grader LLMretrieval_relevance_llm = ChatOpenAI(model=\"gpt-4o\",'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(RetrievalRelevanceGrade, method=\"json_schema\", strict=True)def retrieval_relevance(inputs: dict, outputs: dict) -> bool:    \"\"\"An'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='dict) -> bool:    \"\"\"An evaluator for document relevance\"\"\"    doc_string = \"\\\\n\\\\n\".join(doc.page_content for doc in outputs[\"documents\"])    answer = f\"FACTS: {doc_string}\\\\nQUESTION:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{doc_string}\\\\nQUESTION: {inputs[\\'question\\']}\"    # Run evaluator    grade = retrieval_relevance_llm.invoke([        {\"role\": \"system\", \"content\": retrieval_relevance_instructions},         {\"role\":'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{\"role\": \"user\", \"content\": answer}    ])    return grade[\"relevant\"]import type { EvaluationResult } from \"langsmith/evaluation\";import { z } from \"zod\";// Grade promptconst'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"zod\";// Grade promptconst retrievalRelevanceInstructions = `You are a teacher grading a quiz. You will be given a QUESTION and a set of FACTS provided by the student. Here is the grade criteria to'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Here is the grade criteria to follow:(1) You goal is to identify FACTS that are completely unrelated to the QUESTION(2) If the facts contain ANY keywords or semantic meaning related to the question,'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='related to the question, consider them relevant(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is metRelevance:A relevance value of True means that'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.A relevance value of False means that the FACTS are completely unrelated'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='are completely unrelated to the QUESTION.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='the correct answer at the outset.`const retrievalRelevanceLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='z        .string()        .describe(\"Explain your reasoning for the score\"),      relevant: z        .boolean()        .describe(\"True if the retrieved documents are relevant to the question, False'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='to the question, False otherwise\")    })    .describe(\"Retrieval relevance score for the retrieved documents v.s. the question.\"));async function retrievalRelevance({  inputs,  outputs,}: {  inputs:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='outputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;}): Promise<EvaluationResult> => {  const docString =  outputs.documents.map((doc) => doc.pageContent).join(\"\");  const'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='const answer = `FACTS: ${docString}    QUESTION: ${inputs.question}`      // Run evaluator  const grade = retrievalRelevanceLLM.invoke([{role: \"system\", content: retrievalRelevanceInstructions},'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{role: \"user\", content: answer}])  return grade.relevant};'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Run evaluation\\u200b\\nWe can now kick off our evaluation job with all of our different evaluators.'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScriptdef target(inputs: dict) -> dict:    return rag_bot(inputs[\"question\"])experiment_results = client.evaluate(    target,    data=dataset_name,    evaluators=[correctness, groundedness,'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='groundedness, relevance, retrieval_relevance],    experiment_prefix=\"rag-doc-relevance\",    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},)# Explore results locally as a dataframe if you'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='locally as a dataframe if you have pandas installed# experiment_results.to_pandas()import { evaluate } from \"langsmith/evaluation\";const targetFunc = (inputs: Record<string, any>) => {    return'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='any>) => {    return ragBot(inputs.question)};const experimentResults = await evaluate(targetFunc, {    data: datasetName,    evaluators: [correctness, groundedness, relevance, retrievalRelevance],'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='retrievalRelevance],    experimentPrefix=\"rag-doc-relevance\",    metadata={version: \"LCEL context, gpt-4-0125-preview\"},});'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='You can see an example of what these results look like here: LangSmith link\\nReference code\\u200b'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"Here's a consolidated script with all the above code:PythonTypeScriptfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.vectorstores import InMemoryVectorStorefrom\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='InMemoryVectorStorefrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langsmith import Client, traceablefrom'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='import Client, traceablefrom typing_extensions import Annotated, TypedDict# List of URLs to load documents fromurls = [    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",]# Load documents from the URLsdocs ='), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='documents from the URLsdocs = [WebBaseLoader(url).load() for url in urls]docs_list = [item for sublist in docs for item in sublist]# Initialize a text splitter with specified chunk size and'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='with specified chunk size and overlaptext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(    chunk_size=250, chunk_overlap=0)# Split the documents into chunksdoc_splits ='), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='into chunksdoc_splits = text_splitter.split_documents(docs_list)# Add the document chunks to the \"vector store\" using OpenAIEmbeddingsvectorstore = InMemoryVectorStore.from_documents('), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='documents=doc_splits,    embedding=OpenAIEmbeddings(),)# With langchain we can easily turn any vector store into a retrieval component:retriever = vectorstore.as_retriever(k=6)llm ='), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= ChatOpenAI(model=\"gpt-4o\", temperature=1)# Add decorator so this function is traced in LangSmith@traceable()def rag_bot(question: str) -> dict:    # langchain Retriever will be automatically traced'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='will be automatically traced    docs = retriever.invoke(question)    docs_string = \"\".join(doc.page_content for doc in docs)    instructions = f\"\"\"You are a helpful assistant who is good at analyzing'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"who is good at analyzing source information and answering questions.       Use the following source documents to answer the user's questions.       If you don't know the answer, just say that you\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='the answer, just say that you don\\'t know.       Use three sentences maximum and keep the answer concise.Documents:{docs_string}\"\"\"    # langchain ChatModel will be automatically traced    ai_msg ='), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='traced    ai_msg = llm.invoke(        [            {\"role\": \"system\", \"content\": instructions},            {\"role\": \"user\", \"content\": question},        ],    )    return {\"answer\": ai_msg.content,'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{\"answer\": ai_msg.content, \"documents\": docs}client = Client()# Define the examples for the datasetexamples = [    {        \"inputs\": {\"question\": \"How does the ReAct agent use self-reflection? \"},'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='use self-reflection? \"},        \"outputs\": {\"answer\": \"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='/ reasoning about the tool outputs.\"},    },    {        \"inputs\": {\"question\": \"What are the types of biases that can arise with few-shot prompting?\"},        \"outputs\": {\"answer\": \"The biases that'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{\"answer\": \"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\"},    },    {        \"inputs\": {\"question\": \"What are five'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{\"question\": \"What are five types of adversarial attacks?\"},        \"outputs\": {\"answer\": \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\"},    },]# Create the dataset and examples in LangSmithdataset_name = \"Lilian Weng Blogs Q&A\"if not'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"Lilian Weng Blogs Q&A\"if not client.has_dataset(dataset_name=dataset_name):    dataset = client.create_dataset(dataset_name=dataset_name)    client.create_examples(        dataset_id=dataset.id,'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='dataset_id=dataset.id,        examples=examples    )# Grade output schemaclass CorrectnessGrade(TypedDict):    # Note that the order in the fields are defined is the order in which the model will'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='order in which the model will generate them.    # It is useful to put explanations before responses because it forces the model to think through    # its final response before generating it:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='before generating it:    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    correct: Annotated[bool, ..., \"True if the answer is correct, False otherwise.\"]# Grade'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='False otherwise.\"]# Grade promptcorrectness_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='STUDENT ANSWER. Here is the grade criteria to follow:(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. (2) Ensure that the student answer does'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='that the student answer does not contain any conflicting statements.(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"as it is factually accurate relative to the  ground truth answer.Correctness:A correctness value of True means that the student's answer meets all of the criteria.A correctness value of False means\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='are correct. Avoid simply stating the correct answer at the outset.\"\"\"# Grader LLMgrader_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(    CorrectnessGrade,'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='CorrectnessGrade, method=\"json_schema\", strict=True)def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:    \"\"\"An evaluator for RAG answer accuracy\"\"\"    answers ='), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='accuracy\"\"\"    answers = f\"\"\"\\\\QUESTION: {inputs[\\'question\\']}GROUND TRUTH ANSWER: {reference_outputs[\\'answer\\']}STUDENT ANSWER: {outputs[\\'answer\\']}\"\"\"    # Run evaluator    grade = grader_llm.invoke('), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='grade = grader_llm.invoke(        [            {\"role\": \"system\", \"content\": correctness_instructions},            {\"role\": \"user\", \"content\": answers},        ]    )    return grade[\"correct\"]#'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=')    return grade[\"correct\"]# Grade output schemaclass RelevanceGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    relevant: Annotated[        bool, ...,'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Annotated[        bool, ..., \"Provide the score on whether the answer addresses the question\"    ]# Grade promptrelevance_instructions = \"\"\"You are a teacher grading a quiz. You will be given a'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='a quiz. You will be given a QUESTION and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION(2) Ensure the STUDENT ANSWER'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"Ensure the STUDENT ANSWER helps to answer the QUESTIONRelevance:A relevance value of True means that the student's answer meets all of the criteria.A relevance value of False means that the student's\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='simply stating the correct answer at the outset.\"\"\"# Grader LLMrelevance_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(    RelevanceGrade, method=\"json_schema\", strict=True)#'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='strict=True)# Evaluatordef relevance(inputs: dict, outputs: dict) -> bool:    \"\"\"A simple evaluator for RAG answer helpfulness.\"\"\"    answer = f\"QUESTION: {inputs[\\'question\\']}\\\\nSTUDENT ANSWER:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='ANSWER: {outputs[\\'answer\\']}\"    grade = relevance_llm.invoke(        [            {\"role\": \"system\", \"content\": relevance_instructions},            {\"role\": \"user\", \"content\": answer},        ]    )'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='answer},        ]    )    return grade[\"relevant\"]# Grade output schemaclass GroundedGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    grounded:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='for the score\"]    grounded: Annotated[        bool, ..., \"Provide the score on if the answer hallucinates from the documents\"    ]# Grade promptgrounded_instructions = \"\"\"You are a teacher grading a'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is grounded in the FACTS. (2) Ensure the STUDENT ANSWER'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='(2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.Grounded:A grounded value of True means that the student\\'s answer meets all of the criteria.A'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"meets all of the criteria.A grounded value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"# Grader LLMgrounded_llm = ChatOpenAI(model=\"gpt-4o\",'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(    GroundedGrade, method=\"json_schema\", strict=True)# Evaluatordef groundedness(inputs: dict, outputs: dict) -> bool:    \"\"\"A'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='dict) -> bool:    \"\"\"A simple evaluator for RAG answer groundedness.\"\"\"    doc_string = \"\\\\n\\\\n\".join(doc.page_content for doc in outputs[\"documents\"])    answer = f\"FACTS: {doc_string}\\\\nSTUDENT'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{doc_string}\\\\nSTUDENT ANSWER: {outputs[\\'answer\\']}\"    grade = grounded_llm.invoke(        [            {\"role\": \"system\", \"content\": grounded_instructions},            {\"role\": \"user\", \"content\":'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{\"role\": \"user\", \"content\": answer},        ]    )    return grade[\"grounded\"]# Grade output schemaclass RetrievalRelevanceGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='..., \"Explain your reasoning for the score\"]    relevant: Annotated[        bool,        ...,        \"True if the retrieved documents are relevant to the question, False otherwise\",    ]# Grade'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='False otherwise\",    ]# Grade promptretrieval_relevance_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION and a set of FACTS provided by the student. Here is the grade'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='student. Here is the grade criteria to follow:(1) You goal is to identify FACTS that are completely unrelated to the QUESTION(2) If the facts contain ANY keywords or semantic meaning related to the'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='meaning related to the question, consider them relevant(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is metRelevance:A relevance value of True means'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.A relevance value of False means that the FACTS are completely'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='that the FACTS are completely unrelated to the QUESTION.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='stating the correct answer at the outset.\"\"\"# Grader LLMretrieval_relevance_llm = ChatOpenAI(    model=\"gpt-4o\", temperature=0).with_structured_output(RetrievalRelevanceGrade, method=\"json_schema\",'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='method=\"json_schema\", strict=True)def retrieval_relevance(inputs: dict, outputs: dict) -> bool:    \"\"\"An evaluator for document relevance\"\"\"    doc_string = \"\\\\n\\\\n\".join(doc.page_content for doc in'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='for doc in outputs[\"documents\"])    answer = f\"FACTS: {doc_string}\\\\nQUESTION: {inputs[\\'question\\']}\"    # Run evaluator    grade = retrieval_relevance_llm.invoke(        [            {\"role\":'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='[            {\"role\": \"system\", \"content\": retrieval_relevance_instructions},            {\"role\": \"user\", \"content\": answer},        ]    )    return grade[\"relevant\"]def target(inputs: dict)'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='target(inputs: dict) -> dict:    return rag_bot(inputs[\"question\"])experiment_results = client.evaluate(    target,    data=dataset_name,    evaluators=[correctness, groundedness, relevance,'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='groundedness, relevance, retrieval_relevance],    experiment_prefix=\"rag-doc-relevance\",    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},)# Explore results locally as a dataframe if you'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='locally as a dataframe if you have pandas installed# experiment_results.to_pandas()import { OpenAIEmbeddings, ChatOpenAI } from \"@langchain/openai\";import { MemoryVectorStore } from'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{ MemoryVectorStore } from \"langchain/vectorstores/memory\";import { BrowserbaseLoader } from \"@langchain/community/document_loaders/web/browserbase\";import { traceable } from'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{ traceable } from \"langsmith/traceable\";import { Client } from \"langsmith\";import { evaluate, type EvaluationResult } from \"langsmith/evaluation\";import { z } from \"zod\";// List of URLs to load'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"zod\";// List of URLs to load documents fromconst urls = [    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",]const loader = new BrowserbaseLoader(urls, {    textContent: true,});const docs = await loader.load();const splitter = new'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='splitter = new RecursiveCharacterTextSplitter({    chunkSize: 1000, chunkOverlap: 200});const allSplits = await splitter.splitDocuments(docs);const embeddings = new OpenAIEmbeddings({    model:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='OpenAIEmbeddings({    model: \"text-embedding-3-large\"});const vectorStore = new MemoryVectorStore(embeddings);  // Index chunksawait vectorStore.addDocuments(allSplits)   const llm = new ChatOpenAI({'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='const llm = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 1,})// Add decorator so this function is traced in LangSmithconst ragBot = traceable(    async (question: string) => {        //'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='string) => {        // LangChain retriever will be automatically traced        const retrievedDocs = await vectorStore.similaritySearch(question);        const docsContent = retrievedDocs.map((doc)'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= retrievedDocs.map((doc) => doc.pageContent).join(\"\");                const instructions = `You are a helpful assistant who is good at analyzing source information and answering questions.'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"answering questions.        Use the following source documents to answer the user's questions.        If you don't know the answer, just say that you don't know.        Use three sentences maximum\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Use three sentences maximum and keep the answer concise.        Documents:        ${docsContent}`                const aiMsg = await llm.invoke([            {                role: \"system\",'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='role: \"system\",                content: instructions            },            {                role: \"user\",                content: question            }        ])                return'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='])                return {\"answer\": aiMsg.content, \"documents\": retrievedDocs}    })const client = new Client();// Define the examples for the datasetconst examples = [    [        \"How does the'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= [    [        \"How does the ReAct agent use self-reflection? \",        \"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing /'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='API - and then observing / reasoning about the tool outputs.\",    ],    [        \"What are the types of biases that can arise with few-shot prompting?\",        \"The biases that can arise with'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\",    ],    [        \"What are five types of adversarial attacks?\",'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='adversarial attacks?\",        \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\",'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='(5) Model red-teaming.\",    ]]const [inputs, outputs] = examples.reduce<[Array<{ input: string }>, Array<{ outputs: string }>]>(    ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{ input: item[0] }],    [...outputs, { outputs: item[1] }],    ],    [[], []]);const datasetName = \"Lilian Weng Blogs Q&A\";const dataset = await client.createDataset(datasetName);await'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='client.createExamples({ inputs, outputs, datasetId: dataset.id })// Grade promptconst correctnessInstructions = `You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the grade criteria to follow:(1) Grade the student answers based ONLY on their factual accuracy relative to the ground'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='relative to the ground truth answer. (2) Ensure that the student answer does not contain any conflicting statements.(3) It is OK if the student answer contains more information than the ground truth'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.Correctness:A correctness value of True means that the student's answer meets all of the\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"answer meets all of the criteria.A correctness value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const graderLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),      correct: z        .boolean()'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='z        .boolean()        .describe(\"True if the answer is correct, False otherwise.\")    })    .describe(\"Correctness score for reference answer v.s. generated answer.\"));async function'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='answer.\"));async function correctness({  inputs,  outputs,  referenceOutputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;  referenceOutputs?: Record<string, any>;}):'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Record<string, any>;}): Promise<EvaluationResult> => {  const answer = `QUESTION: ${inputs.question}    GROUND TRUTH ANSWER: ${reference_outputs.answer}    STUDENT ANSWER: ${outputs.answer}`      //'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='${outputs.answer}`      // Run evaluator  const grade = graderLLM.invoke([{role: \"system\", content: correctnessInstructions}, {role: \"user\", content: answer}])  return grade.score};// Grade'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='return grade.score};// Grade promptconst relevanceInstructions = `You are a teacher grading a quiz. You will be given a QUESTION and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='criteria to follow:(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION(2) Ensure the STUDENT ANSWER helps to answer the QUESTIONRelevance:A relevance value of True means that the'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"value of True means that the student's answer meets all of the criteria.A relevance value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const relevanceLLM = new ChatOpenAI({  model:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='for the score\"),      relevant: z        .boolean()        .describe(\"Provide the score on whether the answer addresses the question\")    })    .describe(\"Relevance score for gene\"));async function'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='for gene\"));async function relevance({  inputs,  outputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;}): Promise<EvaluationResult> => {  const answer = `QUESTION:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{  const answer = `QUESTION: ${inputs.question}STUDENT ANSWER: ${outputs.answer}`      // Run evaluator  const grade = relevanceLLM.invoke([{role: \"system\", content: relevanceInstructions}, {role:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{role: \"user\", content: answer}])  return grade.relevant};// Grade promptconst groundedInstructions = `You are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here is the'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is grounded in the FACTS. (2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"information outside the scope of the FACTS.Grounded:A grounded value of True means that the student's answer meets all of the criteria.A grounded value of False means that the student's answer does\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at\"), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='stating the correct answer at the outset.`const groundedLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='z        .string()        .describe(\"Explain your reasoning for the score\"),      grounded: z        .boolean()        .describe(\"Provide the score on if the answer hallucinates from the documents\")'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='from the documents\")    })    .describe(\"Grounded score for the answer from the retrieved documents.\"));async function grounded({  inputs,  outputs,}: {  inputs: Record<string, any>;  outputs:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='any>;  outputs: Record<string, any>;}): Promise<EvaluationResult> => {  const docString =  outputs.documents.map((doc) => doc.pageContent).join(\"\");  const answer = `FACTS: ${docString}    STUDENT'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='${docString}    STUDENT ANSWER: ${outputs.answer}`      // Run evaluator  const grade = groundedLLM.invoke([{role: \"system\", content: groundedInstructions}, {role: \"user\", content: answer}])  return'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='content: answer}])  return grade.grounded};// Grade promptconst retrievalRelevanceInstructions = `You are a teacher grading a quiz. You will be given a QUESTION and a set of FACTS provided by the'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='set of FACTS provided by the student. Here is the grade criteria to follow:(1) You goal is to identify FACTS that are completely unrelated to the QUESTION(2) If the facts contain ANY keywords or'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='facts contain ANY keywords or semantic meaning related to the question, consider them relevant(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='question as long as (2) is metRelevance:A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.A relevance value of'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='relevant.A relevance value of False means that the FACTS are completely unrelated to the QUESTION.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct.'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='and conclusion are correct. Avoid simply stating the correct answer at the outset.`const retrievalRelevanceLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),      relevant: z        .boolean()        .describe(\"True if the retrieved documents are'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='the retrieved documents are relevant to the question, False otherwise\")    })    .describe(\"Retrieval relevance score for the retrieved documents v.s. the question.\"));async function'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='question.\"));async function retrievalRelevance({  inputs,  outputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;}): Promise<EvaluationResult> => {  const docString ='), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='=> {  const docString =  outputs.documents.map((doc) => doc.pageContent).join(\"\");  const answer = `FACTS: ${docString}    QUESTION: ${inputs.question}`      // Run evaluator  const grade ='), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Run evaluator  const grade = retrievalRelevanceLLM.invoke([{role: \"system\", content: retrievalRelevanceInstructions}, {role: \"user\", content: answer}])  return grade.relevant};const targetFunc ='), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='targetFunc = (input: Record<string, any>) => {    return ragBot(inputs.question)};const experimentResults = await evaluate(targetFunc, {    data: datasetName,    evaluators: [correctness,'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='evaluators: [correctness, groundedness, relevance, retrievalRelevance],    experimentPrefix=\"rag-doc-relevance\",    metadata={version: \"LCEL context, gpt-4-0125-preview\"},});Was this page'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='this page helpful?You can leave detailed feedback on GitHub.PreviousEvaluate a chatbotNextRun backtests on a new version of an agentOverviewSetupEnvironmentApplicationDatasetEvaluatorsCorrectness:'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Response vs reference answerRelevance: Response vs inputGroundedness: Response vs retrieved docsRetrieval relevance: Retrieved docs vs inputRun evaluationReference codeCommunityLangChain'), Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='codeCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.')]\n"
     ]
    }
   ],
   "source": [
    "### Load Data  --> Get all the docs --> Divide out text into chunks --> Convert this into vector by using Vector Embeddings -> store in Vector Store DB\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99422e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Evaluate a RAG application | 🦜️🛠️ LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='to AppGet StartedObservabilityEvaluationQuick StartTutorialsEvaluate a chatbotEvaluate a RAG applicationRun backtests on a new version of an agentRunning SWE-bench with LangSmithEvaluate a complex'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='LangSmithEvaluate a complex agentImproving LLM-as-judge evaluators using human feedbackTest a ReAct agent with Pytest/Vitest and LangSmithHow-to GuidesAnalyze a single experimentLog user feedback'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='experimentLog user feedback using the SDKHow to run an evaluationCreating and Managing Datasets in the UIRenaming an experimentAutomatically run evaluators on experimentsHow to manage datasets'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"to manage datasets programmaticallyRunning an evaluation from the prompt playgroundSet up feedback criteriaAnnotate traces and runs inlineHow to evaluate an application's intermediate stepsHow to\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='intermediate stepsHow to version a datasetUse annotation queuesDataset SharingHow to use off-the-shelf evaluators (Python only)How to compare experiment resultsDynamic few shot example selectionHow'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='few shot example selectionHow to evaluate an existing experiment (Python only)How to export filtered traces from experiment to datasetHow to run evals with pytest (beta)Run pairwise evaluationsHow to'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='pairwise evaluationsHow to audit evaluator scoresHow to improve your evaluator with few-shot examplesHow to fetch performance metrics for an experimentHow to use the REST APIHow to upload experiments'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='APIHow to upload experiments run outside of LangSmith with the REST APIHow to run an evaluation asynchronouslyHow to define a custom evaluatorHow to evaluate on a split / filtered view of a'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='a split / filtered view of a datasetHow to evaluate on a specific dataset versionHow to define a target function to evaluateHow to download experiment results as a CSVRun an evaluation with'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='a CSVRun an evaluation with multimodal contentHow to filter experiments in the UIHow to evaluate a langchain runnableHow to evaluate a langgraph graphHow to define an LLM-as-a-judge evaluatorHow to'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='evaluatorHow to run an evaluation locally (Python only)How to return categorical vs numerical metricsHow to simulate multi-turn interactionsHow to return multiple scores in one evaluatorHow to use'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='in one evaluatorHow to use prebuilt evaluatorsHow to handle model rate limitsHow to evaluate with repetitionsHow to define a summary evaluatorHow to run evals with Vitest/Jest (beta)Conceptual'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Vitest/Jest (beta)Conceptual GuidePrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceEvaluationTutorialsEvaluate a RAG applicationOn this pageEvaluate a RAG application'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Key conceptsRAG evaluation | Evaluators | LLM-as-judge evaluators'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Retrieval Augmented Generation (RAG) is a technique that enhances Large Language Models (LLMs) by providing them with relevant external knowledge. It has become one of the most widely used approaches'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='most widely used approaches for building LLM applications.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"This tutorial will show you how to evaluate your RAG applications using LangSmith. You'll learn:\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"How to create test datasets\\nHow to run your RAG application on those datasets\\nHow to measure your application's performance using different evaluation metrics\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Overview\\u200b\\nA typical RAG evaluation workflow consists of three main steps:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Creating a dataset with questions and their expected answers\\nRunning your RAG application on those questions\\nUsing evaluators to measure how well your application performed, looking at factors like:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Answer relevance\\nAnswer accuracy\\nRetrieval quality'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"For this tutorial, we'll create and evaluate a bot that answers questions about a few of Lilian Weng's insightful blog posts.\\nSetup\\u200b\\nEnvironment\\u200b\\nFirst, let's set our environment variables:\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScriptimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = \"YOUR LANGSMITH API KEY\"os.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI API'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= \"YOUR OPENAI API KEY\"process.env.LANGSMITH_TRACING = \"true\";process.env.LANGSMITH_API_KEY = \"YOUR LANGSMITH API KEY\";process.env.OPENAI_API_KEY = \"YOUR OPENAI API KEY\";'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"And install the dependencies we'll need:\\nPythonTypeScriptpip install -U langsmith langchain[openai] langchain-communityyarn add langsmith langchain @langchain/community @langchain/openai\\nApplication\\u200b\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Framework FlexibilityWhile this tutorial uses LangChain, the evaluation techniques and LangSmith functionality demonstrated here work with any framework. Feel free to use your preferred tools and'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='use your preferred tools and libraries.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"In this section, we'll build a basic Retrieval-Augmented Generation (RAG) application.\\nWe'll stick to a simple implementation that:\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"Indexing: chunks and indexes a few of Lilian Weng's blogs in a vector store\\nRetrieval: retrieves those chunks based on the user question\\nGeneration: passes the question and retrieved docs to an LLM.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Indexing and retrieval\\u200b\\nFirst, lets load the blog posts we want to build a chatbot for and index them.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScriptfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsfrom'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitter# List of URLs to load documents fromurls = [    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",]# Load documents from the URLsdocs ='),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='documents from the URLsdocs = [WebBaseLoader(url).load() for url in urls]docs_list = [item for sublist in docs for item in sublist]# Initialize a text splitter with specified chunk size and'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='with specified chunk size and overlaptext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(    chunk_size=250, chunk_overlap=0)# Split the documents into chunksdoc_splits ='),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='into chunksdoc_splits = text_splitter.split_documents(docs_list)# Add the document chunks to the \"vector store\" using OpenAIEmbeddingsvectorstore = InMemoryVectorStore.from_documents('),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='documents=doc_splits,    embedding=OpenAIEmbeddings(),)# With langchain we can easily turn any vector store into a retrieval component:retriever = vectorstore.as_retriever(k=6)import {'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{ OpenAIEmbeddings } from \"@langchain/openai\";import { MemoryVectorStore } from \"langchain/vectorstores/memory\";import { BrowserbaseLoader } from'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{ BrowserbaseLoader } from \"@langchain/community/document_loaders/web/browserbase\";// List of URLs to load documents fromconst urls = [    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",]const loader = new BrowserbaseLoader(urls, {'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='BrowserbaseLoader(urls, {    textContent: true,});const docs = await loader.load();const splitter = new RecursiveCharacterTextSplitter({    chunkSize: 1000, chunkOverlap: 200});const allSplits ='),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='200});const allSplits = await splitter.splitDocuments(docs);const embeddings = new OpenAIEmbeddings({    model: \"text-embedding-3-large\"});const vectorStore = new MemoryVectorStore(embeddings);  //'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='// Index chunksawait vectorStore.addDocuments(allSplits)'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Generation\\u200b\\nWe can now define the generative pipeline.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScriptfrom langchain_openai import ChatOpenAIfrom langsmith import traceablellm = ChatOpenAI(model=\"gpt-4o\", temperature=1)# Add decorator so this function is traced in'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='so this function is traced in LangSmith@traceable()def rag_bot(question: str) -> dict:    # LangChain retriever will be automatically traced    docs = retriever.invoke(question)    docs_string ='),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='docs_string = \"\".join(doc.page_content for doc in docs)    instructions = f\"\"\"You are a helpful assistant who is good at analyzing source information and answering questions.       Use the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"questions.       Use the following source documents to answer the user's questions.       If you don't know the answer, just say that you don't know.       Use three sentences maximum and keep the\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='maximum and keep the answer concise.Documents:{docs_string}\"\"\"    # langchain ChatModel will be automatically traced    ai_msg = llm.invoke([            {\"role\": \"system\", \"content\": instructions},'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"content\": instructions},            {\"role\": \"user\", \"content\": question},        ],    )    return {\"answer\": ai_msg.content, \"documents\": docs}import { ChatOpenAI } from \"@langchain/openai\";import'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"@langchain/openai\";import { traceable } from \"langsmith/traceable\";const llm = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 1,})// Add decorator so this function is traced in LangSmithconst'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='is traced in LangSmithconst ragBot = traceable(    async (question: string) => {        // LangChain retriever will be automatically traced        const retrievedDocs = await'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='const retrievedDocs = await vectorStore.similaritySearch(question);        const docsContent = retrievedDocs.map((doc) => doc.pageContent).join(\"\");                const instructions = `You are a'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"instructions = `You are a helpful assistant who is good at analyzing source information and answering questions        Use the following source documents to answer the user's questions.        If you\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"questions.        If you don't know the answer, just say that you don't know.        Use three sentences maximum and keep the answer concise.        Documents:        ${docsContent}`;\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='const aiMsg = await llm.invoke([            {                role: \"system\",                content: instructions            },            {                role: \"user\",'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='role: \"user\",                content: question            }        ])                return {\"answer\": aiMsg.content, \"documents\": retrievedDocs}    })'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"Dataset\\u200b\\nNow that we've got our application, let's build a dataset to evaluate it. Our dataset will be very simple in this case: we'll have example questions and reference answers.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScriptfrom langsmith import Clientclient = Client()# Define the examples for the datasetexamples = [    {        \"inputs\": {\"question\": \"How does the ReAct agent use self-reflection? \"},'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='use self-reflection? \"},        \"outputs\": {\"answer\": \"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='/ reasoning about the tool outputs.\"},    },    {        \"inputs\": {\"question\": \"What are the types of biases that can arise with few-shot prompting?\"},        \"outputs\": {\"answer\": \"The biases that'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{\"answer\": \"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\"},    },    {        \"inputs\": {\"question\": \"What are five'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{\"question\": \"What are five types of adversarial attacks?\"},        \"outputs\": {\"answer\": \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\"},    }]# Create the dataset and examples in LangSmithdataset_name = \"Lilian Weng Blogs Q&A\"dataset ='),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Weng Blogs Q&A\"dataset = client.create_dataset(dataset_name=dataset_name)client.create_examples(    dataset_id=dataset.id,    examples=examples)import { Client } from \"langsmith\";const client = new'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='client = new Client();// Define the examples for the datasetconst examples = [    [        \"How does the ReAct agent use self-reflection? \",        \"ReAct integrates reasoning and acting, performing'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool outputs.\",    ],    [        \"What are the types of biases that can arise with'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='of biases that can arise with few-shot prompting?\",        \"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\",    ],'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Common token bias.\",    ],    [        \"What are five types of adversarial attacks?\",        \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\",    ]]const [inputs, outputs] = examples.reduce<[Array<{ input: string }>, Array<{ outputs: string }>]>('),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='outputs: string }>]>(    ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],    [...outputs, { outputs: item[1] }],    ],    [[], []]);const datasetName = \"Lilian Weng Blogs Q&A\";const'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"Lilian Weng Blogs Q&A\";const dataset = await client.createDataset(datasetName);await client.createExamples({ inputs, outputs, datasetId: dataset.id })'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Evaluators\\u200b\\nOne way to think about different types of RAG evaluators is as a tuple of what is being evaluated X what its being evaluated against:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Correctness: Response vs reference answer'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Goal: Measure \"how similar/correct is the RAG chain answer, relative to a ground-truth answer\"\\nMode: Requires a ground truth (reference) answer supplied through a dataset'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Evaluator: Use LLM-as-judge to assess answer correctness.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Relevance: Response vs input'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Goal: Measure \"how well does the generated response address the initial user input\"\\nMode: Does not require reference answer, because it will compare the answer to the input question'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Evaluator: Use LLM-as-judge to assess answer relevance, helpfulness, etc.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Groundedness: Response vs retrieved docs'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Goal: Measure \"to what extent does the generated response agree with the retrieved context\"\\nMode: Does not require reference answer, because it will compare the answer to the retrieved context'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Evaluator: Use LLM-as-judge to assess faithfulness, hallucinations, etc.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Retrieval relevance: Retrieved docs vs input'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Goal: Measure \"how relevant are my retrieved results for this query\"\\nMode: Does not require reference answer, because it will compare the question to the retrieved context'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Evaluator: Use LLM-as-judge to assess relevance'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Correctness: Response vs reference answer\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScriptfrom typing_extensions import Annotated, TypedDict# Grade output schemaclass CorrectnessGrade(TypedDict):    # Note that the order in the fields are defined is the order in which the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='is the order in which the model will generate them.    # It is useful to put explanations before responses because it forces the model to think through    # its final response before generating it:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='before generating it:    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    correct: Annotated[bool, ..., \"True if the answer is correct, False otherwise.\"]# Grade'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='False otherwise.\"]# Grade promptcorrectness_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='STUDENT ANSWER. Here is the grade criteria to follow:(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. (2) Ensure that the student answer does'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='that the student answer does not contain any conflicting statements.(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"as it is factually accurate relative to the  ground truth answer.Correctness:A correctness value of True means that the student's answer meets all of the criteria.A correctness value of False means\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='are correct. Avoid simply stating the correct answer at the outset.\"\"\"# Grader LLMgrader_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(CorrectnessGrade, method=\"json_schema\",'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='method=\"json_schema\", strict=True)def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:    \"\"\"An evaluator for RAG answer accuracy\"\"\"    answers = f\"\"\"\\\\QUESTION:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='answers = f\"\"\"\\\\QUESTION: {inputs[\\'question\\']}GROUND TRUTH ANSWER: {reference_outputs[\\'answer\\']}STUDENT ANSWER: {outputs[\\'answer\\']}\"\"\"    # Run evaluator    grade = grader_llm.invoke(['),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= grader_llm.invoke([        {\"role\": \"system\", \"content\": correctness_instructions},         {\"role\": \"user\", \"content\": answers}    ])    return grade[\"correct\"]import type { EvaluationResult }'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='type { EvaluationResult } from \"langsmith/evaluation\";import { z } from \"zod\";// Grade promptconst correctnessInstructions = `You are a teacher grading a quiz. You will be given a QUESTION, the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the grade criteria to follow:(1) Grade the student answers based ONLY on their factual accuracy relative'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='factual accuracy relative to the ground truth answer. (2) Ensure that the student answer does not contain any conflicting statements.(3) It is OK if the student answer contains more information than'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.Correctness:A correctness value of True means that the student's answer meets\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"the student's answer meets all of the criteria.A correctness value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const graderLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),      correct: z        .boolean()'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='z        .boolean()        .describe(\"True if the answer is correct, False otherwise.\")    })    .describe(\"Correctness score for reference answer v.s. generated answer.\"));async function'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='answer.\"));async function correctness({  inputs,  outputs,  referenceOutputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;  referenceOutputs?: Record<string, any>;}):'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Record<string, any>;}): Promise<EvaluationResult> => {  const answer = `QUESTION: ${inputs.question}    GROUND TRUTH ANSWER: ${reference_outputs.answer}    STUDENT ANSWER: ${outputs.answer}`      //'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='${outputs.answer}`      // Run evaluator  const grade = graderLLM.invoke([{role: \"system\", content: correctnessInstructions}, {role: \"user\", content: answer}])  return grade.score};'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Relevance: Response vs input\\u200b\\nThe flow is similar to above, but we simply look at the inputs and outputs without needing the reference_outputs.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"Without a reference answer we can't grade accuracy, but can still grade relevance—as in, did the model address the user's question or not.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScript# Grade output schemaclass RelevanceGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    relevant: Annotated[bool, ..., \"Provide the score'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='..., \"Provide the score on whether the answer addresses the question\"]# Grade promptrelevance_instructions=\"\"\"You are a teacher grading a quiz. You will be given a QUESTION and a STUDENT ANSWER. Here'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION(2) Ensure the STUDENT ANSWER helps to answer the QUESTIONRelevance:A'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"the QUESTIONRelevance:A relevance value of True means that the student's answer meets all of the criteria.A relevance value of False means that the student's answer does not meet all of the\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"#'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='answer at the outset.\"\"\"# Grader LLMrelevance_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(RelevanceGrade, method=\"json_schema\", strict=True)# Evaluatordef relevance(inputs:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='relevance(inputs: dict, outputs: dict) -> bool:    \"\"\"A simple evaluator for RAG answer helpfulness.\"\"\"    answer = f\"QUESTION: {inputs[\\'question\\']}\\\\nSTUDENT ANSWER: {outputs[\\'answer\\']}\"    grade ='),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='grade = relevance_llm.invoke([        {\"role\": \"system\", \"content\": relevance_instructions},         {\"role\": \"user\", \"content\": answer}    ])    return grade[\"relevant\"]import type {'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='type { EvaluationResult } from \"langsmith/evaluation\";import { z } from \"zod\";// Grade promptconst relevanceInstructions = `You are a teacher grading a quiz. You will be given a QUESTION and a'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='be given a QUESTION and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION(2) Ensure the STUDENT ANSWER helps to answer the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"ANSWER helps to answer the QUESTIONRelevance:A relevance value of True means that the student's answer meets all of the criteria.A relevance value of False means that the student's answer does not\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='stating the correct answer at the outset.`const relevanceLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='z        .string()        .describe(\"Explain your reasoning for the score\"),      relevant: z        .boolean()        .describe(\"Provide the score on whether the answer addresses the question\")'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='addresses the question\")    })    .describe(\"Relevance score for gene\"));async function relevance({  inputs,  outputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;}):'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Record<string, any>;}): Promise<EvaluationResult> => {  const answer = `QUESTION: ${inputs.question}STUDENT ANSWER: ${outputs.answer}`      // Run evaluator  const grade = relevanceLLM.invoke([{role:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= relevanceLLM.invoke([{role: \"system\", content: relevanceInstructions}, {role: \"user\", content: answer}])  return grade.relevant};'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Groundedness: Response vs retrieved docs\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Another useful way to evaluate responses without needing reference answers is to check if the response is justified by (or \"grounded in\") the retrieved documents.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScript# Grade output schemaclass GroundedGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    grounded: Annotated[bool, ..., \"Provide the score'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='..., \"Provide the score on if the answer hallucinates from the documents\"]# Grade promptgrounded_instructions = \"\"\"You are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is grounded in the FACTS. (2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"information outside the scope of the FACTS.Grounded:A grounded value of True means that the student's answer meets all of the criteria.A grounded value of False means that the student's answer does\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='stating the correct answer at the outset.\"\"\"# Grader LLM grounded_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(GroundedGrade, method=\"json_schema\", strict=True)#'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='strict=True)# Evaluatordef groundedness(inputs: dict, outputs: dict) -> bool:    \"\"\"A simple evaluator for RAG answer groundedness.\"\"\"    doc_string = \"\\\\n\\\\n\".join(doc.page_content for doc in'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='for doc in outputs[\"documents\"])    answer = f\"FACTS: {doc_string}\\\\nSTUDENT ANSWER: {outputs[\\'answer\\']}\"    grade = grounded_llm.invoke([{\"role\": \"system\", \"content\": grounded_instructions}, {\"role\":'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{\"role\": \"user\", \"content\": answer}])    return grade[\"grounded\"]import type { EvaluationResult } from \"langsmith/evaluation\";import { z } from \"zod\";// Grade promptconst groundedInstructions = `You'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='groundedInstructions = `You are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is grounded in the FACTS.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='is grounded in the FACTS. (2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.Grounded:A grounded value of True means that the student\\'s answer'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"that the student's answer meets all of the criteria.A grounded value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const groundedLLM = new ChatOpenAI({  model: \"gpt-4o\",'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),      grounded: z'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='grounded: z        .boolean()        .describe(\"Provide the score on if the answer hallucinates from the documents\")    })    .describe(\"Grounded score for the answer from the retrieved'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='the answer from the retrieved documents.\"));async function grounded({  inputs,  outputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;}): Promise<EvaluationResult> => {  const'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='=> {  const docString =  outputs.documents.map((doc) => doc.pageContent).join(\"\");  const answer = `FACTS: ${docString}    STUDENT ANSWER: ${outputs.answer}`      // Run evaluator  const grade ='),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Run evaluator  const grade = groundedLLM.invoke([{role: \"system\", content: groundedInstructions}, {role: \"user\", content: answer}])  return grade.grounded};'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Retrieval relevance: Retrieved docs vs input\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScript# Grade output schemaclass RetrievalRelevanceGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    relevant: Annotated[bool, ..., \"True if'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Annotated[bool, ..., \"True if the retrieved documents are relevant to the question, False otherwise\"]# Grade promptretrieval_relevance_instructions = \"\"\"You are a teacher grading a quiz. You will be'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='grading a quiz. You will be given a QUESTION and a set of FACTS provided by the student. Here is the grade criteria to follow:(1) You goal is to identify FACTS that are completely unrelated to the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='completely unrelated to the QUESTION(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant(3) It is OK if the facts have SOME information that is'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='have SOME information that is unrelated to the question as long as (2) is metRelevance:A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='related to the QUESTION and are therefore relevant.A relevance value of False means that the FACTS are completely unrelated to the QUESTION.Explain your reasoning in a step-by-step manner to ensure'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"# Grader LLMretrieval_relevance_llm = ChatOpenAI(model=\"gpt-4o\",'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(RetrievalRelevanceGrade, method=\"json_schema\", strict=True)def retrieval_relevance(inputs: dict, outputs: dict) -> bool:    \"\"\"An'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='dict) -> bool:    \"\"\"An evaluator for document relevance\"\"\"    doc_string = \"\\\\n\\\\n\".join(doc.page_content for doc in outputs[\"documents\"])    answer = f\"FACTS: {doc_string}\\\\nQUESTION:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{doc_string}\\\\nQUESTION: {inputs[\\'question\\']}\"    # Run evaluator    grade = retrieval_relevance_llm.invoke([        {\"role\": \"system\", \"content\": retrieval_relevance_instructions},         {\"role\":'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{\"role\": \"user\", \"content\": answer}    ])    return grade[\"relevant\"]import type { EvaluationResult } from \"langsmith/evaluation\";import { z } from \"zod\";// Grade promptconst'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"zod\";// Grade promptconst retrievalRelevanceInstructions = `You are a teacher grading a quiz. You will be given a QUESTION and a set of FACTS provided by the student. Here is the grade criteria to'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Here is the grade criteria to follow:(1) You goal is to identify FACTS that are completely unrelated to the QUESTION(2) If the facts contain ANY keywords or semantic meaning related to the question,'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='related to the question, consider them relevant(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is metRelevance:A relevance value of True means that'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.A relevance value of False means that the FACTS are completely unrelated'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='are completely unrelated to the QUESTION.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='the correct answer at the outset.`const retrievalRelevanceLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='z        .string()        .describe(\"Explain your reasoning for the score\"),      relevant: z        .boolean()        .describe(\"True if the retrieved documents are relevant to the question, False'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='to the question, False otherwise\")    })    .describe(\"Retrieval relevance score for the retrieved documents v.s. the question.\"));async function retrievalRelevance({  inputs,  outputs,}: {  inputs:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='outputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;}): Promise<EvaluationResult> => {  const docString =  outputs.documents.map((doc) => doc.pageContent).join(\"\");  const'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='const answer = `FACTS: ${docString}    QUESTION: ${inputs.question}`      // Run evaluator  const grade = retrievalRelevanceLLM.invoke([{role: \"system\", content: retrievalRelevanceInstructions},'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{role: \"user\", content: answer}])  return grade.relevant};'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Run evaluation\\u200b\\nWe can now kick off our evaluation job with all of our different evaluators.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='PythonTypeScriptdef target(inputs: dict) -> dict:    return rag_bot(inputs[\"question\"])experiment_results = client.evaluate(    target,    data=dataset_name,    evaluators=[correctness, groundedness,'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='groundedness, relevance, retrieval_relevance],    experiment_prefix=\"rag-doc-relevance\",    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},)# Explore results locally as a dataframe if you'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='locally as a dataframe if you have pandas installed# experiment_results.to_pandas()import { evaluate } from \"langsmith/evaluation\";const targetFunc = (inputs: Record<string, any>) => {    return'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='any>) => {    return ragBot(inputs.question)};const experimentResults = await evaluate(targetFunc, {    data: datasetName,    evaluators: [correctness, groundedness, relevance, retrievalRelevance],'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='retrievalRelevance],    experimentPrefix=\"rag-doc-relevance\",    metadata={version: \"LCEL context, gpt-4-0125-preview\"},});'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='You can see an example of what these results look like here: LangSmith link\\nReference code\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"Here's a consolidated script with all the above code:PythonTypeScriptfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.vectorstores import InMemoryVectorStorefrom\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='InMemoryVectorStorefrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langsmith import Client, traceablefrom'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='import Client, traceablefrom typing_extensions import Annotated, TypedDict# List of URLs to load documents fromurls = [    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",]# Load documents from the URLsdocs ='),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='documents from the URLsdocs = [WebBaseLoader(url).load() for url in urls]docs_list = [item for sublist in docs for item in sublist]# Initialize a text splitter with specified chunk size and'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='with specified chunk size and overlaptext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(    chunk_size=250, chunk_overlap=0)# Split the documents into chunksdoc_splits ='),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='into chunksdoc_splits = text_splitter.split_documents(docs_list)# Add the document chunks to the \"vector store\" using OpenAIEmbeddingsvectorstore = InMemoryVectorStore.from_documents('),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='documents=doc_splits,    embedding=OpenAIEmbeddings(),)# With langchain we can easily turn any vector store into a retrieval component:retriever = vectorstore.as_retriever(k=6)llm ='),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= ChatOpenAI(model=\"gpt-4o\", temperature=1)# Add decorator so this function is traced in LangSmith@traceable()def rag_bot(question: str) -> dict:    # langchain Retriever will be automatically traced'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='will be automatically traced    docs = retriever.invoke(question)    docs_string = \"\".join(doc.page_content for doc in docs)    instructions = f\"\"\"You are a helpful assistant who is good at analyzing'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"who is good at analyzing source information and answering questions.       Use the following source documents to answer the user's questions.       If you don't know the answer, just say that you\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='the answer, just say that you don\\'t know.       Use three sentences maximum and keep the answer concise.Documents:{docs_string}\"\"\"    # langchain ChatModel will be automatically traced    ai_msg ='),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='traced    ai_msg = llm.invoke(        [            {\"role\": \"system\", \"content\": instructions},            {\"role\": \"user\", \"content\": question},        ],    )    return {\"answer\": ai_msg.content,'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{\"answer\": ai_msg.content, \"documents\": docs}client = Client()# Define the examples for the datasetexamples = [    {        \"inputs\": {\"question\": \"How does the ReAct agent use self-reflection? \"},'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='use self-reflection? \"},        \"outputs\": {\"answer\": \"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='/ reasoning about the tool outputs.\"},    },    {        \"inputs\": {\"question\": \"What are the types of biases that can arise with few-shot prompting?\"},        \"outputs\": {\"answer\": \"The biases that'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{\"answer\": \"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\"},    },    {        \"inputs\": {\"question\": \"What are five'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{\"question\": \"What are five types of adversarial attacks?\"},        \"outputs\": {\"answer\": \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\"},    },]# Create the dataset and examples in LangSmithdataset_name = \"Lilian Weng Blogs Q&A\"if not'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"Lilian Weng Blogs Q&A\"if not client.has_dataset(dataset_name=dataset_name):    dataset = client.create_dataset(dataset_name=dataset_name)    client.create_examples(        dataset_id=dataset.id,'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='dataset_id=dataset.id,        examples=examples    )# Grade output schemaclass CorrectnessGrade(TypedDict):    # Note that the order in the fields are defined is the order in which the model will'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='order in which the model will generate them.    # It is useful to put explanations before responses because it forces the model to think through    # its final response before generating it:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='before generating it:    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    correct: Annotated[bool, ..., \"True if the answer is correct, False otherwise.\"]# Grade'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='False otherwise.\"]# Grade promptcorrectness_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='STUDENT ANSWER. Here is the grade criteria to follow:(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. (2) Ensure that the student answer does'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='that the student answer does not contain any conflicting statements.(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"as it is factually accurate relative to the  ground truth answer.Correctness:A correctness value of True means that the student's answer meets all of the criteria.A correctness value of False means\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='are correct. Avoid simply stating the correct answer at the outset.\"\"\"# Grader LLMgrader_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(    CorrectnessGrade,'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='CorrectnessGrade, method=\"json_schema\", strict=True)def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:    \"\"\"An evaluator for RAG answer accuracy\"\"\"    answers ='),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='accuracy\"\"\"    answers = f\"\"\"\\\\QUESTION: {inputs[\\'question\\']}GROUND TRUTH ANSWER: {reference_outputs[\\'answer\\']}STUDENT ANSWER: {outputs[\\'answer\\']}\"\"\"    # Run evaluator    grade = grader_llm.invoke('),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='grade = grader_llm.invoke(        [            {\"role\": \"system\", \"content\": correctness_instructions},            {\"role\": \"user\", \"content\": answers},        ]    )    return grade[\"correct\"]#'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=')    return grade[\"correct\"]# Grade output schemaclass RelevanceGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    relevant: Annotated[        bool, ...,'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Annotated[        bool, ..., \"Provide the score on whether the answer addresses the question\"    ]# Grade promptrelevance_instructions = \"\"\"You are a teacher grading a quiz. You will be given a'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='a quiz. You will be given a QUESTION and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION(2) Ensure the STUDENT ANSWER'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"Ensure the STUDENT ANSWER helps to answer the QUESTIONRelevance:A relevance value of True means that the student's answer meets all of the criteria.A relevance value of False means that the student's\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='simply stating the correct answer at the outset.\"\"\"# Grader LLMrelevance_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(    RelevanceGrade, method=\"json_schema\", strict=True)#'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='strict=True)# Evaluatordef relevance(inputs: dict, outputs: dict) -> bool:    \"\"\"A simple evaluator for RAG answer helpfulness.\"\"\"    answer = f\"QUESTION: {inputs[\\'question\\']}\\\\nSTUDENT ANSWER:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='ANSWER: {outputs[\\'answer\\']}\"    grade = relevance_llm.invoke(        [            {\"role\": \"system\", \"content\": relevance_instructions},            {\"role\": \"user\", \"content\": answer},        ]    )'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='answer},        ]    )    return grade[\"relevant\"]# Grade output schemaclass GroundedGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]    grounded:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='for the score\"]    grounded: Annotated[        bool, ..., \"Provide the score on if the answer hallucinates from the documents\"    ]# Grade promptgrounded_instructions = \"\"\"You are a teacher grading a'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is grounded in the FACTS. (2) Ensure the STUDENT ANSWER'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='(2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.Grounded:A grounded value of True means that the student\\'s answer meets all of the criteria.A'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"meets all of the criteria.A grounded value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.\"\"\"# Grader LLMgrounded_llm = ChatOpenAI(model=\"gpt-4o\",'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(    GroundedGrade, method=\"json_schema\", strict=True)# Evaluatordef groundedness(inputs: dict, outputs: dict) -> bool:    \"\"\"A'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='dict) -> bool:    \"\"\"A simple evaluator for RAG answer groundedness.\"\"\"    doc_string = \"\\\\n\\\\n\".join(doc.page_content for doc in outputs[\"documents\"])    answer = f\"FACTS: {doc_string}\\\\nSTUDENT'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{doc_string}\\\\nSTUDENT ANSWER: {outputs[\\'answer\\']}\"    grade = grounded_llm.invoke(        [            {\"role\": \"system\", \"content\": grounded_instructions},            {\"role\": \"user\", \"content\":'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{\"role\": \"user\", \"content\": answer},        ]    )    return grade[\"grounded\"]# Grade output schemaclass RetrievalRelevanceGrade(TypedDict):    explanation: Annotated[str, ..., \"Explain your'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='..., \"Explain your reasoning for the score\"]    relevant: Annotated[        bool,        ...,        \"True if the retrieved documents are relevant to the question, False otherwise\",    ]# Grade'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='False otherwise\",    ]# Grade promptretrieval_relevance_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION and a set of FACTS provided by the student. Here is the grade'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='student. Here is the grade criteria to follow:(1) You goal is to identify FACTS that are completely unrelated to the QUESTION(2) If the facts contain ANY keywords or semantic meaning related to the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='meaning related to the question, consider them relevant(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is metRelevance:A relevance value of True means'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.A relevance value of False means that the FACTS are completely'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='that the FACTS are completely unrelated to the QUESTION.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='stating the correct answer at the outset.\"\"\"# Grader LLMretrieval_relevance_llm = ChatOpenAI(    model=\"gpt-4o\", temperature=0).with_structured_output(RetrievalRelevanceGrade, method=\"json_schema\",'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='method=\"json_schema\", strict=True)def retrieval_relevance(inputs: dict, outputs: dict) -> bool:    \"\"\"An evaluator for document relevance\"\"\"    doc_string = \"\\\\n\\\\n\".join(doc.page_content for doc in'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='for doc in outputs[\"documents\"])    answer = f\"FACTS: {doc_string}\\\\nQUESTION: {inputs[\\'question\\']}\"    # Run evaluator    grade = retrieval_relevance_llm.invoke(        [            {\"role\":'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='[            {\"role\": \"system\", \"content\": retrieval_relevance_instructions},            {\"role\": \"user\", \"content\": answer},        ]    )    return grade[\"relevant\"]def target(inputs: dict)'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='target(inputs: dict) -> dict:    return rag_bot(inputs[\"question\"])experiment_results = client.evaluate(    target,    data=dataset_name,    evaluators=[correctness, groundedness, relevance,'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='groundedness, relevance, retrieval_relevance],    experiment_prefix=\"rag-doc-relevance\",    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},)# Explore results locally as a dataframe if you'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='locally as a dataframe if you have pandas installed# experiment_results.to_pandas()import { OpenAIEmbeddings, ChatOpenAI } from \"@langchain/openai\";import { MemoryVectorStore } from'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{ MemoryVectorStore } from \"langchain/vectorstores/memory\";import { BrowserbaseLoader } from \"@langchain/community/document_loaders/web/browserbase\";import { traceable } from'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{ traceable } from \"langsmith/traceable\";import { Client } from \"langsmith\";import { evaluate, type EvaluationResult } from \"langsmith/evaluation\";import { z } from \"zod\";// List of URLs to load'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"zod\";// List of URLs to load documents fromconst urls = [    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",]const loader = new BrowserbaseLoader(urls, {    textContent: true,});const docs = await loader.load();const splitter = new'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='splitter = new RecursiveCharacterTextSplitter({    chunkSize: 1000, chunkOverlap: 200});const allSplits = await splitter.splitDocuments(docs);const embeddings = new OpenAIEmbeddings({    model:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='OpenAIEmbeddings({    model: \"text-embedding-3-large\"});const vectorStore = new MemoryVectorStore(embeddings);  // Index chunksawait vectorStore.addDocuments(allSplits)   const llm = new ChatOpenAI({'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='const llm = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 1,})// Add decorator so this function is traced in LangSmithconst ragBot = traceable(    async (question: string) => {        //'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='string) => {        // LangChain retriever will be automatically traced        const retrievedDocs = await vectorStore.similaritySearch(question);        const docsContent = retrievedDocs.map((doc)'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= retrievedDocs.map((doc) => doc.pageContent).join(\"\");                const instructions = `You are a helpful assistant who is good at analyzing source information and answering questions.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"answering questions.        Use the following source documents to answer the user's questions.        If you don't know the answer, just say that you don't know.        Use three sentences maximum\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Use three sentences maximum and keep the answer concise.        Documents:        ${docsContent}`                const aiMsg = await llm.invoke([            {                role: \"system\",'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='role: \"system\",                content: instructions            },            {                role: \"user\",                content: question            }        ])                return'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='])                return {\"answer\": aiMsg.content, \"documents\": retrievedDocs}    })const client = new Client();// Define the examples for the datasetconst examples = [    [        \"How does the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= [    [        \"How does the ReAct agent use self-reflection? \",        \"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing /'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='API - and then observing / reasoning about the tool outputs.\",    ],    [        \"What are the types of biases that can arise with few-shot prompting?\",        \"The biases that can arise with'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\",    ],    [        \"What are five types of adversarial attacks?\",'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='adversarial attacks?\",        \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\",'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='(5) Model red-teaming.\",    ]]const [inputs, outputs] = examples.reduce<[Array<{ input: string }>, Array<{ outputs: string }>]>(    ([inputs, outputs], item) => [    [...inputs, { input: item[0] }],'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{ input: item[0] }],    [...outputs, { outputs: item[1] }],    ],    [[], []]);const datasetName = \"Lilian Weng Blogs Q&A\";const dataset = await client.createDataset(datasetName);await'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='client.createExamples({ inputs, outputs, datasetId: dataset.id })// Grade promptconst correctnessInstructions = `You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. Here is the grade criteria to follow:(1) Grade the student answers based ONLY on their factual accuracy relative to the ground'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='relative to the ground truth answer. (2) Ensure that the student answer does not contain any conflicting statements.(3) It is OK if the student answer contains more information than the ground truth'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.Correctness:A correctness value of True means that the student's answer meets all of the\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"answer meets all of the criteria.A correctness value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const graderLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='\"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),      correct: z        .boolean()'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='z        .boolean()        .describe(\"True if the answer is correct, False otherwise.\")    })    .describe(\"Correctness score for reference answer v.s. generated answer.\"));async function'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='answer.\"));async function correctness({  inputs,  outputs,  referenceOutputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;  referenceOutputs?: Record<string, any>;}):'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Record<string, any>;}): Promise<EvaluationResult> => {  const answer = `QUESTION: ${inputs.question}    GROUND TRUTH ANSWER: ${reference_outputs.answer}    STUDENT ANSWER: ${outputs.answer}`      //'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='${outputs.answer}`      // Run evaluator  const grade = graderLLM.invoke([{role: \"system\", content: correctnessInstructions}, {role: \"user\", content: answer}])  return grade.score};// Grade'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='return grade.score};// Grade promptconst relevanceInstructions = `You are a teacher grading a quiz. You will be given a QUESTION and a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='criteria to follow:(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION(2) Ensure the STUDENT ANSWER helps to answer the QUESTIONRelevance:A relevance value of True means that the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"value of True means that the student's answer meets all of the criteria.A relevance value of False means that the student's answer does not meet all of the criteria.Explain your reasoning in a\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at the outset.`const relevanceLLM = new ChatOpenAI({  model:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='= new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='for the score\"),      relevant: z        .boolean()        .describe(\"Provide the score on whether the answer addresses the question\")    })    .describe(\"Relevance score for gene\"));async function'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='for gene\"));async function relevance({  inputs,  outputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;}): Promise<EvaluationResult> => {  const answer = `QUESTION:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{  const answer = `QUESTION: ${inputs.question}STUDENT ANSWER: ${outputs.answer}`      // Run evaluator  const grade = relevanceLLM.invoke([{role: \"system\", content: relevanceInstructions}, {role:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='{role: \"user\", content: answer}])  return grade.relevant};// Grade promptconst groundedInstructions = `You are a teacher grading a quiz. You will be given FACTS and a STUDENT ANSWER. Here is the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='a STUDENT ANSWER. Here is the grade criteria to follow:(1) Ensure the STUDENT ANSWER is grounded in the FACTS. (2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"information outside the scope of the FACTS.Grounded:A grounded value of True means that the student's answer meets all of the criteria.A grounded value of False means that the student's answer does\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content=\"the student's answer does not meet all of the criteria.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. Avoid simply stating the correct answer at\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='stating the correct answer at the outset.`const groundedLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z    .object({      explanation: z        .string()'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='z        .string()        .describe(\"Explain your reasoning for the score\"),      grounded: z        .boolean()        .describe(\"Provide the score on if the answer hallucinates from the documents\")'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='from the documents\")    })    .describe(\"Grounded score for the answer from the retrieved documents.\"));async function grounded({  inputs,  outputs,}: {  inputs: Record<string, any>;  outputs:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='any>;  outputs: Record<string, any>;}): Promise<EvaluationResult> => {  const docString =  outputs.documents.map((doc) => doc.pageContent).join(\"\");  const answer = `FACTS: ${docString}    STUDENT'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='${docString}    STUDENT ANSWER: ${outputs.answer}`      // Run evaluator  const grade = groundedLLM.invoke([{role: \"system\", content: groundedInstructions}, {role: \"user\", content: answer}])  return'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='content: answer}])  return grade.grounded};// Grade promptconst retrievalRelevanceInstructions = `You are a teacher grading a quiz. You will be given a QUESTION and a set of FACTS provided by the'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='set of FACTS provided by the student. Here is the grade criteria to follow:(1) You goal is to identify FACTS that are completely unrelated to the QUESTION(2) If the facts contain ANY keywords or'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='facts contain ANY keywords or semantic meaning related to the question, consider them relevant(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='question as long as (2) is metRelevance:A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.A relevance value of'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='relevant.A relevance value of False means that the FACTS are completely unrelated to the QUESTION.Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='and conclusion are correct. Avoid simply stating the correct answer at the outset.`const retrievalRelevanceLLM = new ChatOpenAI({  model: \"gpt-4o\",  temperature: 0,}).withStructuredOutput(  z'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='z    .object({      explanation: z        .string()        .describe(\"Explain your reasoning for the score\"),      relevant: z        .boolean()        .describe(\"True if the retrieved documents are'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='the retrieved documents are relevant to the question, False otherwise\")    })    .describe(\"Retrieval relevance score for the retrieved documents v.s. the question.\"));async function'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='question.\"));async function retrievalRelevance({  inputs,  outputs,}: {  inputs: Record<string, any>;  outputs: Record<string, any>;}): Promise<EvaluationResult> => {  const docString ='),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='=> {  const docString =  outputs.documents.map((doc) => doc.pageContent).join(\"\");  const answer = `FACTS: ${docString}    QUESTION: ${inputs.question}`      // Run evaluator  const grade ='),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Run evaluator  const grade = retrievalRelevanceLLM.invoke([{role: \"system\", content: retrievalRelevanceInstructions}, {role: \"user\", content: answer}])  return grade.relevant};const targetFunc ='),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='targetFunc = (input: Record<string, any>) => {    return ragBot(inputs.question)};const experimentResults = await evaluate(targetFunc, {    data: datasetName,    evaluators: [correctness,'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='evaluators: [correctness, groundedness, relevance, retrievalRelevance],    experimentPrefix=\"rag-doc-relevance\",    metadata={version: \"LCEL context, gpt-4-0125-preview\"},});Was this page'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='this page helpful?You can leave detailed feedback on GitHub.PreviousEvaluate a chatbotNextRun backtests on a new version of an agentOverviewSetupEnvironmentApplicationDatasetEvaluatorsCorrectness:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='Response vs reference answerRelevance: Response vs inputGroundedness: Response vs retrieved docsRetrieval relevance: Retrieved docs vs inputRun evaluationReference codeCommunityLangChain'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/evaluation/tutorials/rag', 'title': 'Evaluate a RAG application | 🦜️🛠️ LangSmith', 'description': 'RAG evaluation | Evaluators | LLM-as-judge evaluators', 'language': 'en'}, page_content='codeCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed391d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
